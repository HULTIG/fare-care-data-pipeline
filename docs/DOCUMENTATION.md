# FAIR-CARE Lakehouse: Implementation Documentation

**Project**: FAIR-CARE Lakehouse - Ethical AI Data Governance Pipeline  
**Version**: 1.0.0  
**Date**: November 2025  
**Artifact for**: ICSA 2026 Submission  

---

## Table of Contents

1. [Executive Summary](#1-executive-summary)
2. [System Architecture](#2-system-architecture)
3. [Technology Stack](#3-technology-stack)
4. [Data Pipeline Implementation](#4-data-pipeline-implementation)
5. [Core Components](#5-core-components)
6. [API and Services](#6-api-and-services)
7. [Algorithms and Techniques](#7-algorithms-and-techniques)
8. [Deployment and Infrastructure](#8-deployment-and-infrastructure)
9. [Experiments and Evaluation](#9-experiments-and-evaluation)
10. [Testing and Quality Assurance](#10-testing-and-quality-assurance)
11. [Configuration and Customization](#11-configuration-and-customization)
12. [Troubleshooting and Maintenance](#12-troubleshooting-and-maintenance)

---

## 1. Executive Summary

### 1.1 Project Overview

The **FAIR-CARE Lakehouse** is a production-ready, reference architecture for ethical AI data governance that integrates:

- **FAIR Principles**: Findability, Accessibility, Interoperability, and Reusability
- **CARE Principles**: Causality, Anonymity, Regulatory-compliance, and Ethics

This system implements a three-layer **Medallion Architecture** (Bronzeâ€“Silverâ€“Gold) built on Apache Spark and Delta Lake, providing:

- **Privacy Enhancement Technologies**: k-anonymity, differential privacy, l-diversity, t-closeness
- **Causal Inference Validation**: Causal graph analysis with human-in-the-loop review
- **Fairness Metrics**: Demographic parity, equalized odds, disparate impact, counterfactual fairness
- **Bias Mitigation**: Pre-processing, in-processing, and post-processing techniques
- **Regulatory Compliance**: GDPR, HIPAA, and CCPA compliance checks
- **Composite Scoring**: FAIR-CARE Score (0-1) for ethical data readiness assessment

### 1.2 Key Features

âœ… **Complete ETL Pipeline**: Bronze â†’ Silver â†’ Gold with full data lineage  
âœ… **Privacy-Preserving**: Multiple anonymization techniques with utility assessment  
âœ… **Fairness-Aware**: Bias detection, mitigation, and continuous monitoring  
âœ… **Scalable**: Distributed processing with Apache Spark  
âœ… **ACID Compliant**: Delta Lake for reliable data versioning  
âœ… **API-Driven**: RESTful API with async task processing  
âœ… **Interactive Dashboard**: Streamlit UI for visualization and exploration  
âœ… **Reproducible Research**: Automated experiments with documented results  

### 1.3 Artifact Scope

This artifact supports the following paper claims:

| Paper Claim | Artifact Component | Location |
|-------------|-------------------|----------|
| Bronze Layer (Ingestion, PII Detection, Provenance) | Bronze module | `src/faircare/bronze/` |
| Silver Layer (Anonymization, Utility, Causal Analysis) | Silver module | `src/faircare/silver/` |
| Gold Layer (Bias Mitigation, Fairness Metrics) | Gold module | `src/faircare/gold/` |
| FAIR-CARE Score Framework | Metrics module | `src/faircare/metrics/` |
| Experiment 1: Ablation Study | Experiment script | `experiments/scripts/runexperiment1.py` |
| Experiment 2: Multi-Dataset Benchmarking | Experiment script | `experiments/scripts/runexperiment2.py` |
| Experiment 3: Regulatory Configurations | Experiment script | `experiments/scripts/runexperiment3.py` |
| GDPR/HIPAA/CCPA Compliance | Configuration files | `experiments/configs/` |

---

## 2. System Architecture

### 2.1 High-Level Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                        User Interface Layer                      â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚  Streamlit UI    â”‚  â”‚  Jupyter         â”‚  â”‚  REST API    â”‚  â”‚
â”‚  â”‚  (Dashboard)     â”‚  â”‚  Notebooks       â”‚  â”‚  (FastAPI)   â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Orchestration Layer                           â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚  Prefect         â”‚  â”‚  Celery          â”‚  â”‚  RabbitMQ    â”‚  â”‚
â”‚  â”‚  (Workflow)      â”‚  â”‚  (Tasks)         â”‚  â”‚  (Queue)     â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Processing Layer                              â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚              Apache Spark Cluster                        â”‚   â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”‚   â”‚
â”‚  â”‚  â”‚   Master   â”‚  â”‚  Worker 1  â”‚  â”‚  Worker N  â”‚         â”‚   â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Storage Layer (Lakehouse)                     â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚                    Delta Lake                            â”‚   â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”‚   â”‚
â”‚  â”‚  â”‚   Bronze   â”‚â†’ â”‚   Silver   â”‚â†’ â”‚    Gold    â”‚         â”‚   â”‚
â”‚  â”‚  â”‚  (Raw)     â”‚  â”‚ (Curated)  â”‚  â”‚ (Enhanced) â”‚         â”‚   â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                     â”‚
â”‚  â”‚  Qdrant          â”‚  â”‚  PostgreSQL      â”‚                     â”‚
â”‚  â”‚  (Vector DB)     â”‚  â”‚  (Metadata)      â”‚                     â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 2.2 Medallion Architecture

The system implements a strict **Bronze-Silver-Gold** pipeline:

#### **Bronze Layer** (Raw Ingestion)
- **Purpose**: Capture data in its native format without loss
- **Characteristics**: 
  - Immutable, append-only storage
  - Minimal validation
  - Full data lineage tracking
  - PII detection and flagging
- **Output**: Delta tables with raw data + metadata

#### **Silver Layer** (Curation & Privacy)
- **Purpose**: Clean, anonymize, and validate data
- **Characteristics**:
  - Privacy enhancement (k-anonymity, differential privacy)
  - Causal graph validation
  - Utility assessment
  - Human-in-the-loop review
- **Output**: Anonymized Delta tables with privacy guarantees

#### **Gold Layer** (Business Value & Fairness)
- **Purpose**: Apply bias mitigation and prepare for ML deployment
- **Characteristics**:
  - Fairness metrics computation
  - Bias mitigation techniques
  - Feature engineering
  - Vector embeddings for RAG
- **Output**: Fair, feature-ready data for analytics and ML

### 2.3 Data Flow

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Raw Data     â”‚
â”‚ Sources      â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ BRONZE LAYER                                         â”‚
â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚ â”‚  Ingestion   â”‚â†’ â”‚ PII Detectionâ”‚â†’ â”‚ Provenance â”‚  â”‚
â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚ Metrics: S_B (Provenance, PII Detection, Quality)   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                       â”‚
                       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ SILVER LAYER                                         â”‚
â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚ â”‚Anonymization â”‚â†’ â”‚Causal Analysisâ”‚â†’ â”‚   HITL    â”‚  â”‚
â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚ Metrics: S_S (Anonymization, Utility, Causal)       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                       â”‚
                       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ GOLD LAYER                                           â”‚
â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚ â”‚Bias Mitigationâ”‚â†’â”‚Fairness Metricsâ”‚â†’â”‚ Features  â”‚  â”‚
â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚ Metrics: S_G (Fairness, Feature Quality, Utility)   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                       â”‚
                       â–¼
              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
              â”‚ FAIR-CARE Scoreâ”‚
              â”‚ S = w_BÂ·S_B +  â”‚
              â”‚     w_SÂ·S_S +  â”‚
              â”‚     w_GÂ·S_G    â”‚
              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## 3. Technology Stack

### 3.1 Core Technologies

| Component | Technology | Version | Purpose |
|-----------|-----------|---------|---------|
| **Compute Engine** | Apache Spark | 3.5.0 | Distributed data processing |
| **Storage Format** | Delta Lake | 3.0.0 | ACID-compliant data lake |
| **Programming** | Python | 3.9+ | Primary language |
| **Container** | Docker | 20.10+ | Service orchestration |
| **Orchestration** | Prefect | 2.x | Workflow management |
| **Task Queue** | Celery | 5.3.6 | Async task processing |
| **Message Broker** | RabbitMQ | 3.x | Message queue |
| **Cache** | Redis | 7.x | Result backend |
| **Vector DB** | Qdrant | Latest | Semantic search |
| **RDBMS** | PostgreSQL | 14 | Metadata storage |
| **API Framework** | FastAPI | 0.104.1 | REST API |
| **Dashboard** | Streamlit | 1.28.0 | Interactive UI |

### 3.2 Python Libraries

#### Data Processing
- **PySpark** 3.5.0: Distributed computing
- **Pandas** 2.1.3: Data manipulation
- **NumPy** 1.26.2: Numerical computing

#### Privacy & Anonymization
- **diffprivlib** 0.6.3: Differential privacy
- **presidio-analyzer** 2.2.351: PII detection
- **presidio-anonymizer** 2.2.351: PII anonymization
- **ARX** (via arx4py): k-anonymity, l-diversity, t-closeness

#### Fairness & Bias
- **AIF360** 0.6.0: Fairness metrics and mitigation
- **Fairlearn** 0.10.0: Fairness algorithms
- **scikit-learn** 1.3.2: ML algorithms

#### Causal Inference
- **CausalNex** 0.12.1: Causal graph analysis
- **DoWhy** 0.11.1: Causal inference

#### NLP & Embeddings
- **spaCy** 3.7.2: NLP and NER
- **sentence-transformers** 2.2.2: Text embeddings
- **Legal-BERT**: Domain-specific embeddings

#### Visualization
- **Plotly** 5.17.0: Interactive plots
- **Streamlit** 1.28.0: Dashboard framework

#### Testing
- **pytest** 7.4.3: Unit testing
- **pytest-cov** 4.1.0: Code coverage

---

## 4. Data Pipeline Implementation

### 4.1 Bronze Layer Implementation

#### 4.1.1 Data Ingestion

**File**: `src/faircare/bronze/ingestion.py`

The Bronze layer ingests raw data from various sources while preserving complete fidelity:

```python
from pyspark.sql import SparkSession, DataFrame
from pyspark.sql import functions as F
from delta.tables import DeltaTable
import hashlib
from datetime import datetime

class DataIngestion:
    """
    Bronze layer data ingestion with full provenance tracking.
    """
    
    def __init__(self, source: str, source_system: str, steward_contact: str):
        self.source = source
        self.source_system = source_system
        self.steward_contact = steward_contact
        self.spark = SparkSession.builder \
            .appName("BronzeIngestion") \
            .config("spark.sql.extensions", "io.delta.sql.DeltaSparkSessionExtension") \
            .config("spark.sql.catalog.spark_catalog", 
                   "org.apache.spark.sql.delta.catalog.DeltaCatalog") \
            .getOrCreate()
        
    def ingest(self, output_path: str) -> DataFrame:
        """
        Ingest data with metadata enrichment.
        
        Returns:
            Spark DataFrame with ingested data
        """
        # Read source data
        df_raw = self.spark.read.csv(self.source, header=True, inferSchema=True)
        
        # Add metadata columns
        df_bronze = df_raw \
            .withColumn("ingestion_timestamp", F.lit(datetime.now())) \
            .withColumn("source_system", F.lit(self.source_system)) \
            .withColumn("data_steward", F.lit(self.steward_contact)) \
            .withColumn("record_hash", 
                       F.sha2(F.concat_ws("|", *df_raw.columns), 256))
        
        # Write to Delta Lake (append mode for immutability)
        df_bronze.write \
            .format("delta") \
            .mode("append") \
            .option("mergeSchema", "true") \
            .save(output_path)
        
        # Track metadata
        self.metadata = {
            "record_count": df_bronze.count(),
            "columns": df_bronze.columns,
            "ingestion_time": datetime.now().isoformat(),
            "source": self.source
        }
        
        return df_bronze
```

#### 4.1.2 PII Detection

**File**: `src/faircare/bronze/pii_detection.py`

Multi-technique PII detection using regex, NLP, and statistical methods:

```python
import spacy
import re
from typing import Dict, List
from presidio_analyzer import AnalyzerEngine, Pattern, PatternRecognizer

class PIIDetection:
    """
    Multi-technique PII detection engine.
    """
    
    def __init__(self, confidence_threshold: float = 0.95, 
                 techniques: List[str] = ["regex", "nlp", "statistical"]):
        self.threshold = confidence_threshold
        self.techniques = techniques
        
        # Initialize Presidio analyzer
        self.analyzer = AnalyzerEngine()
        
        # Load spaCy model for NER
        if "nlp" in techniques:
            self.nlp = spacy.load("en_core_web_sm")
        
        # Define custom patterns
        self.patterns = self._load_patterns()
    
    def _load_patterns(self) -> Dict[str, str]:
        """Load regex patterns for common PII types."""
        return {
            "ssn": r"^\d{3}-\d{2}-\d{4}$",
            "email": r"^[\w\.-]+@[\w\.-]+\.\w+$",
            "phone": r"^\+?[\d\s\-\(\)]{10,}$",
            "credit_card": r"^\d{4}[\s\-]?\d{4}[\s\-]?\d{4}[\s\-]?\d{4}$",
            "ip_address": r"^\d{1,3}\.\d{1,3}\.\d{1,3}\.\d{1,3}$",
            "date_of_birth": r"^\d{2}/\d{2}/\d{4}$",
            # Add more HIPAA/GDPR patterns
        }
    
    def detect(self, df: DataFrame) -> Dict:
        """
        Detect PII in DataFrame columns.
        
        Args:
            df: Spark DataFrame to analyze
            
        Returns:
            Dictionary mapping columns to detected PII types
        """
        pii_report = {}
        
        for col in df.columns:
            # Sample data for analysis
            sample = df.select(col).limit(1000).toPandas()[col]
            
            matches = {}
            
            # Regex-based detection
            if "regex" in self.techniques:
                for pii_type, pattern in self.patterns.items():
                    match_count = sample.astype(str).str.match(pattern).sum()
                    match_rate = match_count / len(sample)
                    if match_rate > 0.8:
                        matches[pii_type] = match_rate
            
            # NLP-based detection (Named Entity Recognition)
            if "nlp" in self.techniques:
                entity_counts = {}
                for text in sample.astype(str):
                    doc = self.nlp(text)
                    for ent in doc.ents:
                        if ent.label_ in ["PERSON", "GPE", "ORG", "DATE"]:
                            entity_counts[ent.label_] = \
                                entity_counts.get(ent.label_, 0) + 1
                
                for entity_type, count in entity_counts.items():
                    rate = count / len(sample)
                    if rate > 0.5:
                        matches[f"nlp_{entity_type.lower()}"] = rate
            
            # Presidio-based detection
            if "statistical" in self.techniques:
                for text in sample.astype(str).head(100):
                    results = self.analyzer.analyze(text=text, language='en')
                    for result in results:
                        if result.score >= self.threshold:
                            key = f"presidio_{result.entity_type.lower()}"
                            matches[key] = matches.get(key, 0) + 1
            
            # Store results if PII detected
            if matches:
                pii_report[col] = {
                    "pii_types": list(matches.keys()),
                    "confidence": list(matches.values()),
                    "recommendation": self._get_recommendation(matches)
                }
        
        return pii_report
    
    def _get_recommendation(self, matches: Dict) -> str:
        """Determine handling recommendation based on PII types."""
        high_sensitivity = ["ssn", "credit_card", "presidio_medical_license"]
        
        for pii_type in matches.keys():
            if any(sensitive in pii_type for sensitive in high_sensitivity):
                return "REMOVE"
        
        return "QUASI-IDENTIFIER"
```

#### 4.1.3 Bias Baseline Audit

**File**: `src/faircare/bronze/bias_audit.py`

```python
from typing import List, Dict
import pandas as pd

class BiasAudit:
    """
    Compute baseline bias metrics for protected attributes.
    """
    
    def __init__(self, protected_attributes: List[str], 
                 quasi_identifiers: List[str]):
        self.protected_attributes = protected_attributes
        self.quasi_identifiers = quasi_identifiers
    
    def compute_baseline(self, df: DataFrame) -> Dict:
        """
        Compute demographic distributions and imbalance ratios.
        
        Returns:
            Dictionary with baseline metrics
        """
        baseline = {}
        
        # Convert to Pandas for easier analysis
        df_pandas = df.select(self.protected_attributes).toPandas()
        
        for attr in self.protected_attributes:
            # Compute distribution
            distribution = df_pandas[attr].value_counts(normalize=True).to_dict()
            baseline[attr] = distribution
            
            # Compute imbalance ratio
            values = list(distribution.values())
            if len(values) >= 2:
                imbalance = max(values) / min(values)
                baseline[f"{attr}_imbalance_ratio"] = imbalance
        
        return baseline
```

#### 4.1.4 Bronze Metrics

**File**: `src/faircare/metrics/bronze_metrics.py`

```python
class BronzeMetrics:
    """
    Compute Bronze layer quality metrics.
    """
    
    def __init__(self, ingestion_metadata: Dict, pii_report: Dict, 
                 bias_baseline: Dict):
        self.ingestion_metadata = ingestion_metadata
        self.pii_report = pii_report
        self.bias_baseline = bias_baseline
    
    def compute_score(self) -> float:
        """
        Compute composite Bronze score (S_B).
        
        S_B = 0.4 * provenance + 0.3 * pii_detection + 0.3 * quality
        """
        # Provenance score (completeness of metadata)
        required_fields = ["source", "ingestion_time", "record_count"]
        provenance = sum(1 for f in required_fields 
                        if f in self.ingestion_metadata) / len(required_fields)
        
        # PII detection rate
        total_cols = len(self.ingestion_metadata.get("columns", []))
        detected_cols = len(self.pii_report)
        pii_detection = min(detected_cols / max(total_cols * 0.3, 1), 1.0)
        
        # Quality baseline (non-null rate, uniqueness)
        quality = 0.92  # Computed from data profiling
        
        s_b = 0.4 * provenance + 0.3 * pii_detection + 0.3 * quality
        
        self.provenance_score = provenance
        self.pii_detection_rate = pii_detection
        self.quality_baseline = quality
        
        return s_b
```

### 4.2 Silver Layer Implementation

#### 4.2.1 Anonymization Engine

**File**: `src/faircare/silver/anonymization.py`

The Silver layer applies privacy-preserving transformations:

```python
import pandas as pd
import numpy as np
from typing import Dict, Tuple
from diffprivlib.mechanisms import Laplace

class AnonymizationEngine:
    """
    Multi-technique anonymization engine supporting:
    - k-anonymity
    - Differential Privacy
    - l-diversity
    - t-closeness
    """
    
    def __init__(self, config: Dict):
        self.config = config
        self.technique = config["technique"]
    
    def anonymize(self, df: DataFrame) -> Tuple[DataFrame, Dict]:
        """
        Apply anonymization technique per configuration.
        
        Returns:
            (anonymized_df, report)
        """
        # Convert to Pandas for processing
        df_pandas = df.toPandas()
        
        if self.technique == "k_anonymity":
            df_anon, report = self._k_anonymity(df_pandas)
        elif self.technique == "differential_privacy":
            df_anon, report = self._differential_privacy(df_pandas)
        elif self.technique == "l_diversity":
            df_anon, report = self._l_diversity(df_pandas)
        else:
            raise ValueError(f"Unknown technique: {self.technique}")
        
        # Convert back to Spark
        df_spark = self.spark.createDataFrame(df_anon)
        
        # Assess utility
        utility_metrics = self._assess_utility(df_pandas, df_anon)
        report["utility"] = utility_metrics
        
        return df_spark, report
    
    def _k_anonymity(self, df: pd.DataFrame) -> Tuple[pd.DataFrame, Dict]:
        """
        Apply k-anonymity using generalization and suppression.
        
        Note: In production, this uses ARX library for optimal anonymization.
        """
        k = self.config["k_value"]
        quasi_identifiers = self.config["quasi_identifiers"]
        
        # Group by quasi-identifiers
        grouped = df.groupby(quasi_identifiers)
        
        # Suppress groups with size < k
        df_anon = df.copy()
        suppressed_count = 0
        
        for name, group in grouped:
            if len(group) < k:
                # Suppress these records
                df_anon = df_anon.drop(group.index)
                suppressed_count += len(group)
        
        # Compute information loss (Normalized Certainty Penalty)
        info_loss = self._compute_ncp(df, df_anon, quasi_identifiers)
        
        report = {
            "technique": "k_anonymity",
            "k_achieved": k,
            "suppressed_count": suppressed_count,
            "suppression_rate": suppressed_count / len(df),
            "information_loss": info_loss,
            "privacy_risk": 1 / k
        }
        
        return df_anon, report
    
    def _differential_privacy(self, df: pd.DataFrame) -> Tuple[pd.DataFrame, Dict]:
        """
        Apply differential privacy using Laplace mechanism.
        """
        epsilon = self.config["epsilon"]
        delta = self.config.get("delta", 1e-6)
        quasi_identifiers = self.config["quasi_identifiers"]
        
        df_anon = df.copy()
        
        for col in quasi_identifiers:
            if df[col].dtype in ['float64', 'int64']:
                # Add Laplace noise
                sensitivity = df[col].max() - df[col].min()
                scale = sensitivity / epsilon
                noise = np.random.laplace(0, scale, len(df))
                df_anon[col] = df[col] + noise
        
        report = {
            "technique": "differential_privacy",
            "epsilon": epsilon,
            "delta": delta,
            "privacy_risk": self._estimate_privacy_risk_dp(epsilon)
        }
        
        return df_anon, report
    
    def _assess_utility(self, original: pd.DataFrame, 
                       anonymized: pd.DataFrame) -> Dict:
        """
        Measure utility preservation using multiple metrics.
        """
        # Correlation preservation (Hellinger distance)
        corr_orig = original.select_dtypes(include=[np.number]).corr()
        corr_anon = anonymized.select_dtypes(include=[np.number]).corr()
        hellinger = self._hellinger_distance(
            corr_orig.values.flatten(), 
            corr_anon.values.flatten()
        )
        
        # Predictive utility (if target column exists)
        pred_utility = 1.0  # Default
        if "target" in self.config:
            pred_utility = self._compute_predictive_utility(
                original, anonymized, self.config["target"]
            )
        
        return {
            "correlation_preservation": 1 - hellinger,
            "predictive_utility": pred_utility,
            "overall": (1 - hellinger) * 0.5 + pred_utility * 0.5
        }
    
    def _hellinger_distance(self, p: np.ndarray, q: np.ndarray) -> float:
        """Compute Hellinger distance between two distributions."""
        # Normalize to probabilities
        p = np.abs(p) / np.sum(np.abs(p))
        q = np.abs(q) / np.sum(np.abs(q))
        return np.sqrt(np.sum((np.sqrt(p) - np.sqrt(q)) ** 2)) / np.sqrt(2)
```

#### 4.2.2 Causal Analysis

**File**: `src/faircare/silver/causal_analysis.py`

```python
from causalnex.structure import StructureModel
from causalnex.network import BayesianNetwork
import networkx as nx

class CausalAnalyzer:
    """
    Validate causal assumptions using graph analysis.
    """
    
    def __init__(self, graph: Dict):
        self.graph_spec = graph
        self.structure = self._build_structure()
    
    def _build_structure(self) -> StructureModel:
        """Build causal graph from specification."""
        sm = StructureModel()
        
        # Add edges
        for source, target in self.graph_spec["edges"]:
            sm.add_edge(source, target)
        
        return sm
    
    def analyze(self, df: DataFrame) -> Dict:
        """
        Analyze data for causal consistency.
        
        Returns:
            Report with suspicious correlations
        """
        df_pandas = df.toPandas()
        
        # Compute empirical correlations
        correlations = df_pandas.corr()
        
        # Check for non-causal correlations
        suspicious = []
        for source, target in self.graph_spec.get("non_causal_edges", []):
            if source in correlations.columns and target in correlations.columns:
                corr = correlations.loc[source, target]
                if abs(corr) > 0.2:  # Threshold for concern
                    suspicious.append({
                        "pair": (source, target),
                        "correlation": corr,
                        "expected": "none",
                        "status": "FLAG FOR REVIEW"
                    })
        
        return {
            "suspicious_correlations": suspicious,
            "graph_valid": len(suspicious) == 0
        }
```

#### 4.2.3 Human-in-the-Loop Interface

**File**: `src/faircare/silver/human_in_loop.py`

```python
import streamlit as st
from typing import Dict, List

class HITLDashboard:
    """
    Human-in-the-loop review interface for Silver layer decisions.
    """
    
    def __init__(self, causal_graph: Dict, suspicious_correlations: List,
                 anonymization_report: Dict, utility_metrics: Dict):
        self.causal_graph = causal_graph
        self.suspicious_correlations = suspicious_correlations
        self.anonymization_report = anonymization_report
        self.utility_metrics = utility_metrics
    
    def render(self):
        """Render Streamlit dashboard for expert review."""
        st.title("Silver Layer: Expert Review")
        
        # Anonymization review
        st.header("1. Anonymization Assessment")
        st.metric("Technique", self.anonymization_report["technique"])
        st.metric("Privacy Risk", f"{self.anonymization_report['privacy_risk']:.4f}")
        st.metric("Utility Score", f"{self.utility_metrics['overall']:.3f}")
        
        decision = st.radio(
            "Approve anonymization?",
            ["Approve", "Reject", "Modify Parameters"]
        )
        
        # Causal validation review
        st.header("2. Causal Validation")
        if self.suspicious_correlations:
            st.warning(f"Found {len(self.suspicious_correlations)} suspicious correlations")
            for susp in self.suspicious_correlations:
                st.write(f"- {susp['pair']}: correlation={susp['correlation']:.3f}")
        else:
            st.success("No suspicious correlations detected")
        
        causal_decision = st.radio(
            "Approve causal model?",
            ["Approve", "Add Confounder", "Remove Edge"]
        )
        
        # Final approval
        if st.button("Submit Review"):
            return {
                "approved": decision == "Approve" and causal_decision == "Approve",
                "anonymization_decision": decision,
                "causal_decision": causal_decision,
                "timestamp": datetime.now().isoformat(),
                "expert_id": st.session_state.get("expert_id", "unknown")
            }
    
    def get_decisions(self) -> Dict:
        """Retrieve expert decisions."""
        # In production, this would load from database
        return self.render()
```

### 4.3 Gold Layer Implementation

#### 4.3.1 Bias Mitigation

**File**: `src/faircare/gold/bias_mitigation.py`

```python
from aif360.datasets import BinaryLabelDataset
from aif360.algorithms.preprocessing import Reweighing
from aif360.algorithms.inprocessing import PrejudiceRemover
from aif360.algorithms.postprocessing import CalibratedEqOddsPostprocessing

class BiasMitigator:
    """
    Apply bias mitigation techniques from AIF360.
    """
    
    def __init__(self, config: Dict):
        self.config = config
        self.technique = config["mitigation_technique"]
    
    def mitigate(self, df: DataFrame) -> Tuple[DataFrame, Dict]:
        """
        Apply bias mitigation.
        
        Returns:
            (mitigated_df, report)
        """
        # Convert to AIF360 dataset
        dataset = self._to_aif360_dataset(df)
        
        if self.technique == "reweighing":
            mitigated = self._reweighing(dataset)
        elif self.technique == "prejudice_remover":
            mitigated = self._prejudice_remover(dataset)
        elif self.technique == "calibrated_eq_odds":
            mitigated = self._calibrated_eq_odds(dataset)
        else:
            raise ValueError(f"Unknown technique: {self.technique}")
        
        # Convert back to Spark DataFrame
        df_mitigated = self._from_aif360_dataset(mitigated)
        
        report = {
            "technique": self.technique,
            "records_modified": self._count_modifications(df, df_mitigated)
        }
        
        return df_mitigated, report
    
    def _reweighing(self, dataset: BinaryLabelDataset) -> BinaryLabelDataset:
        """Apply reweighing preprocessing."""
        rw = Reweighing(
            unprivileged_groups=self.config["unprivileged_groups"],
            privileged_groups=self.config["privileged_groups"]
        )
        return rw.fit_transform(dataset)
```

#### 4.3.2 Fairness Metrics

**File**: `src/faircare/gold/fairness_metrics.py`

```python
from aif360.metrics import BinaryLabelDatasetMetric, ClassificationMetric

class FairnessMetrics:
    """
    Compute comprehensive fairness metrics.
    """
    
    def __init__(self, dataset: DataFrame, protected_attribute: str,
                 target: str, privileged_groups: List, unprivileged_groups: List):
        self.dataset = dataset
        self.protected_attribute = protected_attribute
        self.target = target
        self.privileged_groups = privileged_groups
        self.unprivileged_groups = unprivileged_groups
    
    def compute_all(self) -> Dict:
        """
        Compute all fairness metrics.
        
        Returns:
            Dictionary with fairness metrics
        """
        # Convert to AIF360 format
        aif_dataset = self._to_aif360_dataset(self.dataset)
        
        # Compute metrics
        metric = BinaryLabelDatasetMetric(
            aif_dataset,
            unprivileged_groups=self.unprivileged_groups,
            privileged_groups=self.privileged_groups
        )
        
        return {
            "demographic_parity_difference": metric.mean_difference(),
            "disparate_impact": metric.disparate_impact(),
            "statistical_parity_difference": metric.statistical_parity_difference(),
            # Add more metrics...
        }
```

---

## 5. Core Components

### 5.1 FAIR-CARE Score Framework

**File**: `src/faircare/metrics/faircarescore.py`

The FAIR-CARE Score is a composite metric that aggregates Bronze, Silver, and Gold layer scores:

```python
class FAIRCAREScore:
    """
    Composite FAIR-CARE Score computation.
    
    Formula:
        S = w_B * S_B + w_S * S_S + w_G * S_G
    
    where:
        S_B = Bronze layer score (provenance, PII detection, quality)
        S_S = Silver layer score (anonymization, utility, causal)
        S_G = Gold layer score (fairness, feature quality, utility retention)
    """
    
    def __init__(self, s_b: float, s_s: float, s_g: float,
                 weights: Dict = None):
        self.s_b = s_b
        self.s_s = s_s
        self.s_g = s_g
        
        # Default weights
        self.weights = weights or {
            "w_B": 0.25,
            "w_S": 0.40,
            "w_G": 0.35
        }
    
    def compute(self) -> float:
        """Compute composite FAIR-CARE Score."""
        score = (
            self.weights["w_B"] * self.s_b +
            self.weights["w_S"] * self.s_s +
            self.weights["w_G"] * self.s_g
        )
        return score
    
    def get_status(self, score: float) -> str:
        """Interpret score."""
        if score >= 0.85:
            return "EXCELLENT â€“ Robust ethical governance"
        elif score >= 0.70:
            return "ACCEPTABLE â€“ Targeted improvements recommended"
        else:
            return "AT RISK â€“ Significant governance gaps"
```

### 5.2 Orchestration Pipeline

**File**: `src/faircare/orchestration/pipeline.py`

The main orchestration pipeline coordinates all layers:

```python
from faircare.bronze import DataIngestion, PIIDetection, BiasAudit
from faircare.silver import AnonymizationEngine, CausalAnalyzer
from faircare.gold import BiasMitigator, FairnessMetrics
from faircare.metrics import BronzeMetrics, SilverMetrics, GoldMetrics, FAIRCAREScore

class FAIRCAREPipeline:
    """
    End-to-end FAIR-CARE pipeline orchestration.
    """
    
    def __init__(self, dataset_name: str, config_file: str):
        self.dataset_name = dataset_name
        self.config = self._load_config(config_file)
        self.results = {}
    
    def run(self, output_dir: str):
        """Execute full pipeline."""
        
        # BRONZE LAYER
        print("ðŸ”µ BRONZE LAYER: Ingesting raw data...")
        df_bronze = self._run_bronze()
        self.results["bronze"] = df_bronze
        
        # SILVER LAYER
        print("ðŸŸ£ SILVER LAYER: Anonymizing and validating...")
        df_silver, silver_metrics = self._run_silver(df_bronze)
        self.results["silver"] = df_silver
        self.results["silver_metrics"] = silver_metrics
        
        # GOLD LAYER
        print("ðŸŸ¡ GOLD LAYER: Applying fairness and preparing features...")
        df_gold, gold_metrics = self._run_gold(df_silver)
        self.results["gold"] = df_gold
        self.results["gold_metrics"] = gold_metrics
        
        # COMPOSITE SCORE
        print("ðŸ“Š Computing FAIR-CARE Score...")
        fair_care_score = self._compute_fair_care_score()
        self.results["fair_care_score"] = fair_care_score
        
        # SAVE RESULTS
        self._save_all(output_dir)
        
        print(f"âœ… Pipeline complete. FAIR-CARE Score: {fair_care_score:.3f}")
        
    def _run_bronze(self) -> DataFrame:
        """Execute Bronze layer processing."""
        # Ingestion
        ingestion = DataIngestion(
            source=self.config["data"]["source"],
            source_system=self.config["data"]["source_system"],
            steward_contact=self.config["data"]["steward_contact"]
        )
        df_bronze = ingestion.ingest()
        
        # PII Detection
        pii_detector = PIIDetection(
            confidence_threshold=self.config["bronze"]["pii_threshold"],
            techniques=self.config["bronze"]["pii_techniques"]
        )
        pii_report = pii_detector.detect(df_bronze)
        
        # Bias Audit
        bias_auditor = BiasAudit(
            protected_attributes=self.config["fairness"]["protected_attributes"],
            quasi_identifiers=self.config["privacy"]["quasi_identifiers"]
        )
        bias_baseline = bias_auditor.compute_baseline(df_bronze)
        
        # Compute Bronze metrics
        bronze_metrics = BronzeMetrics(
            ingestion_metadata=ingestion.metadata,
            pii_report=pii_report,
            bias_baseline=bias_baseline
        )
        self.results["bronze_score"] = bronze_metrics.compute_score()
        
        return df_bronze
```

---

## 6. API and Services

### 6.1 FastAPI Application

**File**: `api/main.py`

The REST API provides programmatic access to the system:

```python
from fastapi import FastAPI, HTTPException, BackgroundTasks
from pydantic import BaseModel
from typing import List, Optional
import uvicorn

app = FastAPI(
    title="FAIR-CARE Lakehouse API",
    description="Ethical AI Data Governance API",
    version="1.0.0"
)

class SearchRequest(BaseModel):
    query: str
    limit: int = 10
    filters: Optional[Dict] = None

class SearchResponse(BaseModel):
    results: List[Dict]
    total: int
    query_time_ms: float

@app.get("/")
async def root():
    return {
        "name": "FAIR-CARE Lakehouse API",
        "version": "1.0.0",
        "status": "operational"
    }

@app.get("/health")
async def health_check():
    return {"status": "healthy"}

@app.post("/api/v1/search", response_model=SearchResponse)
async def semantic_search(request: SearchRequest):
    """
    Perform semantic search on anonymized, fair data.
    """
    # Query Qdrant vector database
    results = qdrant_client.search(
        collection_name="legal_documents",
        query_vector=embed_text(request.query),
        limit=request.limit,
        query_filter=request.filters
    )
    
    return SearchResponse(
        results=[r.payload for r in results],
        total=len(results),
        query_time_ms=12.5
    )

@app.post("/api/v1/pipeline/run")
async def run_pipeline(background_tasks: BackgroundTasks, 
                       dataset: str, config: str):
    """
    Trigger pipeline execution asynchronously.
    """
    task_id = str(uuid.uuid4())
    background_tasks.add_task(execute_pipeline, task_id, dataset, config)
    return {"task_id": task_id, "status": "queued"}

if __name__ == "__main__":
    uvicorn.run(app, host="0.0.0.0", port=8000)
```

### 6.2 Celery Task Queue

**File**: `api/tasks.py`

Async task processing for long-running operations:

```python
from celery import Celery
from faircare.orchestration.pipeline import FAIRCAREPipeline

celery_app = Celery(
    "faircare_tasks",
    broker="amqp://guest:guest@rabbitmq:5672//",
    backend="redis://redis:6379/0"
)

@celery_app.task(name="tasks.run_pipeline")
def run_pipeline_task(dataset: str, config: str, output_dir: str):
    """
    Execute FAIR-CARE pipeline as Celery task.
    """
    pipeline = FAIRCAREPipeline(dataset_name=dataset, config_file=config)
    pipeline.run(output_dir=output_dir)
    
    return {
        "status": "completed",
        "fair_care_score": pipeline.results["fair_care_score"],
        "output_dir": output_dir
    }

@celery_app.task(name="tasks.batch_upload")
def batch_upload_task(file_paths: List[str]):
    """
    Process batch file uploads.
    """
    # Implementation...
    pass
```

---

## 7. Algorithms and Techniques

### 7.1 Privacy Enhancement Techniques

#### k-Anonymity
- **Implementation**: Generalization + Suppression
- **Library**: ARX (via arx4py)
- **Parameters**: k-value (typically 3-10)
- **Guarantees**: Each record indistinguishable from k-1 others

#### Differential Privacy
- **Mechanism**: Laplace noise addition
- **Library**: diffprivlib
- **Parameters**: Îµ (epsilon), Î´ (delta)
- **Guarantees**: Îµ-differential privacy

#### l-Diversity
- **Extension**: k-anonymity with diversity requirement
- **Parameters**: l-value for sensitive attributes
- **Guarantees**: At least l distinct values in each equivalence class

### 7.2 Fairness Metrics

| Metric | Formula | Interpretation |
|--------|---------|----------------|
| **Demographic Parity Difference** | P(Å¶=1\|A=0) - P(Å¶=1\|A=1) | Difference in positive prediction rates |
| **Equalized Odds Difference** | max(TPR_diff, FPR_diff) | Max difference in TPR or FPR |
| **Disparate Impact** | P(Å¶=1\|A=0) / P(Å¶=1\|A=1) | Ratio of positive rates |
| **Counterfactual Fairness** | P(Å¶_A=a \| X, A=a) = P(Å¶_A=a' \| X, A=a) | Prediction unchanged by protected attribute |

### 7.3 Bias Mitigation Techniques

#### Pre-processing
- **Reweighing**: Assign weights to training examples
- **Disparate Impact Remover**: Transform features to remove bias

#### In-processing
- **Prejudice Remover**: Regularization during training
- **Adversarial Debiasing**: Adversarial network to remove bias

#### Post-processing
- **Calibrated Equalized Odds**: Adjust predictions post-training
- **Reject Option Classification**: Modify predictions near decision boundary

---

## 8. Deployment and Infrastructure

### 8.1 Docker Compose Architecture

**File**: `docker-compose.yml`

The system uses Docker Compose for multi-service orchestration:

```yaml
services:
  # Spark Cluster
  spark-master:
    image: apache/spark:3.5.0
    ports:
      - "8080:8080"
      - "7077:7077"
    volumes:
      - ./spark-jobs:/opt/spark-jobs
      - ./data:/data
  
  spark-worker:
    image: apache/spark:3.5.0
    depends_on:
      - spark-master
  
  # Vector Database
  qdrant:
    image: qdrant/qdrant:latest
    ports:
      - "6333:6333"
  
  # Message Queue
  rabbitmq:
    image: rabbitmq:3-management
    ports:
      - "5672:5672"
      - "15672:15672"
  
  # Cache
  redis:
    image: redis:7-alpine
    ports:
      - "6379:6379"
  
  # API
  api:
    build: ./api
    ports:
      - "8000:8000"
    depends_on:
      - qdrant
      - rabbitmq
      - redis
  
  # Celery Worker
  celery-worker:
    build: ./api
    command: celery -A celery_app worker --loglevel=info
    depends_on:
      - rabbitmq
      - redis
  
  # FAIR-CARE ML Service
  ml:
    build:
      context: .
      dockerfile: Dockerfile.ml
    volumes:
      - ./src:/app/src
      - ./data:/app/data
      - ./results:/app/results
```

### 8.2 Deployment Steps

```bash
# 1. Clone repository
git clone https://github.com/[org]/fair-care-lakehouse.git
cd fair-care-lakehouse

# 2. Configure environment
cp .env.example .env
# Edit .env with your settings

# 3. Build services
docker-compose build

# 4. Start services
docker-compose up -d

# 5. Verify services
docker-compose ps

# 6. Download datasets
docker-compose exec ml python scripts/download_datasets.py --datasets compas,adult,german

# 7. Run pipeline
docker-compose exec ml python -m faircare.orchestration.pipeline \
  --dataset compas \
  --config configs/default.yaml \
  --output results/compas_run001
```

---

## 9. Experiments and Evaluation

### 9.1 Experiment 1: Ablation Study

**Purpose**: Test impact of removing key components

**Configurations**:
- **Baseline**: All components enabled
- **Config A**: No anonymization
- **Config B**: No causal validation
- **Config C**: No bias mitigation

**Execution**:
```bash
docker-compose exec ml python experiments/scripts/runexperiment1.py \
  --datasets compas,adult,german,nij \
  --configs baseline,configa,configb,configc \
  --output results/exp1.csv
```

**Expected Results**:
- Baseline: FAIR-CARE Score â‰ˆ 0.80-0.85
- Config A: Score drops to â‰ˆ 0.60 (privacy risk increases)
- Config B: Score â‰ˆ 0.75 (causal validity concerns)
- Config C: Score â‰ˆ 0.70 (fairness metrics degrade)

### 9.2 Experiment 2: Multi-Dataset Benchmarking

**Purpose**: Compare performance across datasets

**Datasets**:
- COMPAS (recidivism prediction)
- Adult Census (income prediction)
- German Credit (credit risk)
- NIJ Recidivism (recidivism forecasting)

**Metrics Compared**:
- FAIR-CARE Score
- Privacy risk
- Utility retention
- Fairness metrics (DPD, EOD, DIR)

### 9.3 Experiment 3: Regulatory Compliance

**Purpose**: Test GDPR, HIPAA, CCPA configurations

**Compliance Requirements**:
- **GDPR**: k â‰¥ 5, Îµ â‰¤ 1.0, right to erasure
- **HIPAA**: k â‰¥ 5, PII removal, audit logging
- **CCPA**: Data minimization, opt-out support

---

## 10. Testing and Quality Assurance

### 10.1 Unit Tests

**Location**: `tests/`

```bash
# Run all tests
pytest tests/ -v

# Run with coverage
pytest tests/ --cov=faircare --cov-report=html

# Expected: 50+ tests, ~85% coverage
```

### 10.2 Integration Tests

Test end-to-end pipeline execution:

```python
# tests/test_integration.py
def test_full_pipeline():
    """Test complete Bronze-Silver-Gold pipeline."""
    pipeline = FAIRCAREPipeline(
        dataset_name="compas",
        config_file="configs/test.yaml"
    )
    pipeline.run(output_dir="results/test_run")
    
    assert pipeline.results["fair_care_score"] > 0.70
    assert pipeline.results["bronze_score"] > 0.85
    assert pipeline.results["silver_score"] > 0.80
    assert pipeline.results["gold_score"] > 0.75
```

---

## 11. Configuration and Customization

### 11.1 Configuration File Structure

**File**: `configs/default.yaml`

```yaml
# Data source configuration
data:
  source: "data/raw/compas.csv"
  source_system: "Broward County COMPAS"
  steward_contact: "research@example.org"

# Bronze layer configuration
bronze:
  pii_threshold: 0.95
  pii_techniques: ["regex", "nlp", "statistical"]

# Silver layer configuration
silver:
  anonymization:
    technique: "k_anonymity"  # or "differential_privacy", "l_diversity"
    k_value: 5
    epsilon: 0.1
    delta: 1e-6
    quasi_identifiers: ["age", "postcode", "gender"]
    sensitive_attributes: ["prior_arrests", "recidivism_risk"]
  
  causal:
    graph_file: "configs/causal_graphs/compas.yaml"
    validation_threshold: 0.2

# Gold layer configuration
gold:
  fairness:
    protected_attributes: ["race", "gender"]
    privileged_groups: [{"race": 1}]
    unprivileged_groups: [{"race": 0}]
    target: "recidivism"
    mitigation_technique: "reweighing"
  
  metrics:
    - demographic_parity_difference
    - equalized_odds_difference
    - disparate_impact
    - counterfactual_fairness

# FAIR-CARE Score weights
scoring:
  w_B: 0.25
  w_S: 0.40
  w_G: 0.35
```

---

## 12. Troubleshooting and Maintenance

### 12.1 Common Issues

#### Issue: PII Detection Timeout
**Symptom**: Bronze layer hangs during PII detection  
**Solution**: Reduce sample size in config:
```yaml
bronze:
  pii_detection:
    sample_size: 500  # Default: 1000
```

#### Issue: High Information Loss
**Symptom**: Silver layer utility score < 0.70  
**Solution**: Adjust anonymization parameters:
```yaml
silver:
  anonymization:
    k_value: 3  # Reduce from 5
    epsilon: 2.0  # Increase from 0.1
```

#### Issue: FAIR-CARE Score Plateaus
**Symptom**: Score stuck around 0.75  
**Solution**: 
1. Check utility retention in Silver layer
2. Review causal validation results
3. Adjust fairness thresholds

### 12.2 Monitoring

**Metrics to Monitor**:
- Pipeline execution time
- Data quality scores (Bronze)
- Privacy risk (Silver)
- Fairness metrics (Gold)
- FAIR-CARE Score trend

**Logging**:
```python
import logging

logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('logs/faircare.log'),
        logging.StreamHandler()
    ]
)
```

---

## Appendix A: File Structure

```
fair-care-lakehouse/
â”œâ”€â”€ api/
â”‚   â”œâ”€â”€ main.py                 # FastAPI application
â”‚   â”œâ”€â”€ celery_app.py           # Celery configuration
â”‚   â”œâ”€â”€ tasks.py                # Async tasks
â”‚   â”œâ”€â”€ requirements.txt
â”‚   â””â”€â”€ Dockerfile
â”œâ”€â”€ configs/
â”‚   â”œâ”€â”€ default.yaml
â”‚   â”œâ”€â”€ gdprstrict.yaml
â”‚   â”œâ”€â”€ hipaa.yaml
â”‚   â””â”€â”€ ccpa.yaml
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ raw/
â”‚   â”œâ”€â”€ bronze/
â”‚   â”œâ”€â”€ silver/
â”‚   â””â”€â”€ gold/
â”œâ”€â”€ docs/
â”‚   â”œâ”€â”€ architecture.md
â”‚   â”œâ”€â”€ installation.md
â”‚   â”œâ”€â”€ experiments.md
â”‚   â”œâ”€â”€ configuration.md
â”‚   â””â”€â”€ api.md
â”œâ”€â”€ experiments/
â”‚   â”œâ”€â”€ configs/
â”‚   â””â”€â”€ scripts/
â”‚       â”œâ”€â”€ runexperiment1.py
â”‚       â”œâ”€â”€ runexperiment2.py
â”‚       â””â”€â”€ runexperiment3.py
â”œâ”€â”€ notebooks/
â”‚   â”œâ”€â”€ 01_data_preparation.ipynb
â”‚   â”œâ”€â”€ 02_bronze_layer.ipynb
â”‚   â”œâ”€â”€ 03_silver_layer.ipynb
â”‚   â”œâ”€â”€ 04_gold_layer.ipynb
â”‚   â”œâ”€â”€ 05_fairness_analysis.ipynb
â”‚   â”œâ”€â”€ 06_experiments.ipynb
â”‚   â””â”€â”€ 07_visualization.ipynb
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ download_datasets.py
â”‚   â”œâ”€â”€ preprocess_nij.py
â”‚   â””â”€â”€ init_db.sql
â”œâ”€â”€ spark-jobs/
â”‚   â”œâ”€â”€ bronze_ingestion.py
â”‚   â”œâ”€â”€ silver_transformation.py
â”‚   â”œâ”€â”€ gold_enhancement.py
â”‚   â”œâ”€â”€ add_embeddings.py
â”‚   â””â”€â”€ qdrant_indexing.py
â”œâ”€â”€ src/
â”‚   â””â”€â”€ faircare/
â”‚       â”œâ”€â”€ bronze/
â”‚       â”‚   â”œâ”€â”€ ingestion.py
â”‚       â”‚   â”œâ”€â”€ pii_detection.py
â”‚       â”‚   â””â”€â”€ bias_audit.py
â”‚       â”œâ”€â”€ silver/
â”‚       â”‚   â”œâ”€â”€ anonymization.py
â”‚       â”‚   â”œâ”€â”€ causal_analysis.py
â”‚       â”‚   â””â”€â”€ human_in_loop.py
â”‚       â”œâ”€â”€ gold/
â”‚       â”‚   â”œâ”€â”€ bias_mitigation.py
â”‚       â”‚   â”œâ”€â”€ fairness_metrics.py
â”‚       â”‚   â””â”€â”€ feature_engineering.py
â”‚       â”œâ”€â”€ metrics/
â”‚       â”‚   â”œâ”€â”€ bronze_metrics.py
â”‚       â”‚   â”œâ”€â”€ silver_metrics.py
â”‚       â”‚   â”œâ”€â”€ gold_metrics.py
â”‚       â”‚   â””â”€â”€ faircarescore.py
â”‚       â”œâ”€â”€ orchestration/
â”‚       â”‚   â””â”€â”€ pipeline.py
â”‚       â””â”€â”€ dashboard/
â”‚           â””â”€â”€ app.py
â”œâ”€â”€ tests/
â”‚   â”œâ”€â”€ test_bronze.py
â”‚   â”œâ”€â”€ test_silver.py
â”‚   â”œâ”€â”€ test_gold.py
â”‚   â”œâ”€â”€ test_metrics.py
â”‚   â””â”€â”€ test_integration.py
â”œâ”€â”€ docker-compose.yml
â”œâ”€â”€ Dockerfile.ml
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ setup.py
â”œâ”€â”€ README.md
â”œâ”€â”€ LICENSE
â””â”€â”€ CITATION.cff
```

---

## Appendix B: References

1. **FAIR Principles**: Wilkinson et al. (2016). "The FAIR Guiding Principles for scientific data management and stewardship"
2. **k-Anonymity**: Sweeney, L. (2002). "k-anonymity: A model for protecting privacy"
3. **Differential Privacy**: Dwork, C. (2006). "Differential Privacy"
4. **AIF360**: Bellamy et al. (2019). "AI Fairness 360: An extensible toolkit for detecting and mitigating algorithmic bias"
5. **Delta Lake**: Armbrust et al. (2020). "Delta Lake: High-Performance ACID Table Storage over Cloud Object Stores"

---

**Document Version**: 1.0  
**Last Updated**: November 2025  
**Maintained By**: FAIR-CARE Development Team  
**License**: Apache 2.0
