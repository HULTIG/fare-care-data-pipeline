Welcome to the deep dive. Forget for a moment the usual technical battles we discuss. Optimizing cloud latency scaling microservices or maximizing transactional throughput today we are focusing on something with. Much higher stakes algorithmic decision making in context that. That determine individual liberty we're really in the high risk world here justice public safety correctional systems exactly we're talking about ai systems that are guiding parole eligibility setting recidivism risk scores or even determining rehabilitation pathways and when you have that level of impact the architecture underpinning those systems is no longer a purely technical concern it really stops being about just. Maximizing speed it has to be about maximizing due process in fairness absolutely as our sources argue in these high-risk domains the system design is the justice system for all intents and purposes that is exactly our mission today we have been handed the blueprints for a truly cutting-edge concept. The fair kerr lake house this is a detailed reference architecture for data pipelines designed to. Sort of. Baked ethics privacy and fairness directly into the foundation it's a total redesign not just an add-on it's a complete redesign of how sensitive data flows through an organization and crucially into a machine learning model and the sources they really acknowledge this. Central almost brutal conflict that architects are facing right now which is what exactly how do you use that sensitive highly granular data things like supervision case information the detailed demographics you need for a highly active model. How do you use that while at the same time guaranteeing the anonymity of the individual preventing the amplification of historical bias and on top of all that strictly adhering to global regulations like gdpr in europe and hypo here in the us the tension is. If the whole ball game it is and this architecture is an attempt to resolve that tension but in code and the solution isn't just bolting on a compliance checklist at the end of the process. That's the old way. This is a fundamental set of data governance princ. So we're all familiar with the standard fare principles you know findability accessibility interoperability reusability. But this new architecture extends those. It has four critical ethical pillars which they call care. Care okay what does that stand for it stands for causality anonymity regulatory compliance and ethics. It's essentially an eight-point contract for ethical data management and the goal really is to transform ai from this kind of. High risk unscrewable black box. Into a transparent accountable and governable decision support system especially in domains where human lives are so directly affected that's the whole point okay let's unpack this architecture layer by layer. I think we have to start by fully appreciating the crisis that. You know required this kind of foundational shift in the first place so we have to start by recognizing the problem. The sources they officially categorize it as high-risk domains a judicial correctional public safety contact. Exactly. In these areas the ais output isn't a product recommendation it's not suggesting a movie it's offer a score that dictates life. A lower recidivism score could mean early parole. A high one might mean delayed access to rehabilitation or you know years of extended supervision and the fundamental challenge for these systems before we even get to the architecture is something you mentioned earlier the historical bias trap yes. The fuel these systems have run on for decades is to put it bluntly inherently toxic. What do you mean by that the data used to train these models reflects historical realities not some underlying objective truth about crime so it's a reflection of society's actions precis. If for decades historical policing and judicial patterns show that suit and demographics were subject to higher arrest rates or longer sentences or heavier surveillance then the data reflects that the data reflects those systemic inequities. So when you feed that historical data into a machine learning model the model doesn't miraculously correct for the bias the pattern it internalizes it it codifies it. It learns to reproduce the pattern of historical discrimination. And often with terrifying efficiency. And this is exactly why applying fx as a quote. Post hoc audit lei. Is so insufficient it's too little. By the time in ethics team gets around to finding the bias. The model has already learned it's dangerous lesson. The data is. It's already poisoned exactly. You can't just review an output score and somehow fix the underlying systemic problem that created it. The system architecture itself has to prevent that poisoning from happening in the first. Which means the rule of the software architect has fundamentally shifted. They are no longer just focus on operational metrics you know like up time or scalability they have to become what the source is called an ethical guardian an ethical guardians someone who is responsible for ensuring that fairness privacy and ethical accountability are mandated by the systems design from the very beginning so if traditional etl pipelines extract transform load or the old way of doing things why did they fail so spectacularly here well traditional etl it's designed for efficiency and movement it's great at moving data around cleaning it up preparing it for storage but it's ethically dumb. It is it has no built-in structural checks for bios amplification it completely fails to preserve what's called causal integrity meaning it just treats all features equally whether there are caused or just a correlation and doesn't have the governance mechon. Critically know. It lacks the integrated governance needed to handle these complex evolving legal constraints of gdpr or hepa. In anything close to real time the data flows into the model prep environment already carrying all of that structural bias with it okay so let's really unpack the core tensions the architect has to manage. First there's the big one. The utility versus privacy fairness. I mean to build a highly accurate useful recidivism forecasting model the machine learning researchers they want granular detail they want everything they want everything they need sensitive attributes. Full prior criminal history case management notes supervision info and crucially demographics like race gender and age which are often proxies for really important systemic factors all that granularity is what gives the model its high predictive power exactly but that level of detail clashes immediately and violently with privacy risks absolutely correctional injustice data is without a doubt among the most sensitive government data that exists. And standard pseudonym just removing a name and a social security number it's completely insufficient the source is warn about these combinations of attributes these quasi identifiers yes and they can be enough to identify an individual just think about it if you combine a very precise release date with a zip code and then add the offense type in a small town that could be one person. In a localized data set that combination can be unique or nearly. And the second an individual could be re-identified or even if they can just be singled out within the data set you have failed the most basic privacy compliance rules. Okay so that brings us to what might be the most complex challenge this architecture tackles. The causal risk. This is a classic distinction between correlation and causation that has you know plague machine learning since day one it really has and machine learning models are at their core extraordinary correlation. That's what they're designed to do find patterns that predict an outcome efficiently. Recidivism the model might find a really strong undeniable correlation between say. Living in a specific high poverty census tract or a pma. And the architects temptation where the data scientists temptation is to use that pma feature because it's significantly boost. The models predictive score precisely you get a better accuracy metric but this architecture argues very rigorously that the pma is not the cause it's a confounder the confounder the mma might only correlate so strongly because it's a proxy for the true underlying systemic fact. Lack of educational opportunities or and this is a critical one a history of heavy over-policing that naturally results in higher rest rates in that specific area so if the model includes puma. It effectively codifies spatial and by extension racial bi. Even if you took the race attribute itself out of the data exactly and that is such a key point so the architecture has to. Actively intervene in the future selection process it has a distinguish between that spurious correlation that confounder like geography and a true cause like a lack of access to effective rehabilitation of the noise before the model ever sees it yes. And that is a massive philosophical shift in how we think about data preparation so because traditional etl is as we've established ethically and legally insaf. The solution propose is this causal ethical lake house right and this structure in adopts the scalable modular data lake house paradigm which you often hear called the medallion pattern and then it fuses it with these very specific modules designed for causal inference and differential prive. The medallion model for those who might be unfamiliar it's essentially a three-tier data organization system. Bronze for raw silver for refined. In gold for curated data and in this architecture each of those layers becomes a dedicated ethical. Okay let's start at the beginning then layer one the bronze layer this is the wrong slayer you can think of it as the quarantine zone the front door the front door. Data from the source systems like a correctional case management. It lands here first. And key technologies like delta lake. Are used here to ensure immutable ingestion and very strict schema validation why is immutability so important here it gives you a reliable auditable unchangeable system of record. It establishes the data's initial provenance. You can always trace back to the exact state of the data as it arrived which is critical for legal challenges and audits okay. And legally speaking the data here is only pseudonym. You mentioned this distinction is crucial under regulations like gdpr it is absolutely crucial it's the first line of defense. But it is nowhere near sufficient for full compliance. Pseudonym just means replacing direct identifiers names ssms with a hashed value usually using a salted. So the date is secured but but legally the cryptographic link back to the individual still exists. The gdpr is very clear on this. Data remains personal data and is subject to the full weight of all compliance obligations. We have to go further to achieve true anonymity. So what key actions are happening right after ingestion in the bronze layer the first action is immediate. Automated scanning for pii or personally identifiable information so a tool is just. Crawling the new data exactly. Tools like microsoft presidio are deployed to scan the data identify columns and tag them according to their sensitivity level. All the directly identifying columns are immediately sodomised in this layer is also generating a score yes the bronze score. The scores of metric that measures data quality. The completeness of the lineage tracking and the success rate of that pii detection the target is 95% accuracy or better. It establishes a baseline for governance for the rest of the pipeline okay so moving up the stack. We hit the real transformation zone layer 2 the silver layer you said this is where the magic. In the regulatory compliance really happens this is the refined privacy layer the key legal and technical transformation happens right here. The data moves from being merely pseudonym. To becoming epsilon differentially priv. And that's the big job that's the hugely. This is the architectural attempt to achieve what's known as legal safe harbor status by proving mathematically that the risk of re-identify any single person is miniscule what are the main functions happening in this layer. There are a few big ones de-identification is number one. Then synthetic data generation or sdg. Also rigorous anonymity validation using checks like deland anonymity and finally the preliminary steps for causality like constructing the initial variable set for the causal discovery phase that happens later so you're actively destroying the one-to-one link. To the real person. But trying to preserve the aggregate statistical utility the models need that is the tightrope walk you have to preserve the patterns without preserving the people it finally at the top we reach layer 3. The gold layer this is a curated fair layer this is the data that's ready for direct model consumption the privacy state here is fully aggregated anonymized. If functions is the final validated feature store. And its primary job shifts from transformation to validation and mitigation so here you have rigorous causal discovery filtering the feature set using the mark off blanket applying bias mitigation techniques like rewang and then finally preparing the data for model training or for more advanced systems like retrieval augment to generation or rg and watching over this entire process running vertically across all three layers. Is the control playing that control plane is the operationalization of those care principles. It's the central nervous system of the whole are. It manages all the metadata it tracks comprehensive audit logs for every single transformation and critically it tracks and enforces the privacy budget. Epsilon. This is where compliance is code really lives. It uses tools like mlflow and unity catalog to enforce all the rules and automatically generate compliance art. The data protection impact assessment or dpaa for every new data set that gets. Architectural innovation meets the. The hard reality of legal constraint. Simple masking is clearly not enough for genuine legal protection. Across international boundaries. We need a robust anonymization engine that's exactly right the architecture mandates advanced privacy enhancing technologies pets and it specifically focuses on two. Differential privacy and synthetic data jenner. And the goal is to achieve. True anonim. A true anonymity. Meaning the data transformation has to be irreversible and make re-identification functionally impossible. Let's start with differential privacy. Explain for us why this mathematical guarantee is so critical especially when you're dealing with high risk data like this. Differential privacy or dp it provides a quantifiable mathematical guarantee of privacy. Data. Dpa works by adding a precisely calculated amount of random statistical noise usually drawn from a laplace or a gaussian distribution to the aggregate results of any query. So if i ask what's the average recidivism rate for this county the noise added is small enough that you can still drive highly accurate statistical trends you'll get an answer that's very close to the true. But it's large enough to obscure the contribution of any single individual in that data set exactly the formal definition guarantees that any calculation you perform in the data set will yield almost the exact same result whether or not any single individual's record was included in that calculation. So you can't tell if i'm in the data or. Alright you cannot deduce whether a specific person is in the data set. Nor can you deduce the value of a sensitive attribute for that person even if you know everything else about them. And this is all controlled by the privacy budget. This isn't just a compliance number you're saying it's a critical operational constraint on the system it is the operational. The epsilon budget tracks the cumulative privacy laws over time and across all data operations what does that mean cumulative well every time you query the silver layer data you consume a little bit of that budget. You leak a tiny tiny amount of information. If too many queries are run that accumulated leakage starts to add up and once the budget is exhausted. Mathematically lockdown it cannot be queried further and typical high risk thresholds might mandate that epsilon stays below 1.0 or even lower. This directly addresses the gdpr is deep concern about inference risk. By putting a hard cap on how much information can be extracted cumulative. Okay so next up in the engine is synthetic data generation. Sdg. This seems to be the prioritize method for getting both high utility and low risk at the same time sdg is a real paradigm shift instead of sharing real noisy data you share statistically identical fake data. How does. Degenerative model often a conditional generative adversarial network or ctg in. Is trained to learn the high dimensional statistical distribution of the real data. It learns all the complex correlations between. A fence type prior history and so on and once it learns those patterns it then generates entirely new fictional records that adhered all those complex relationship. So the data analyst or the model gets a data set that has the same statistical properties for training but they're literally no real people in it that's the core advantage and it's a huge one. Since the synthetic data breaks that one-to-one linked with any real individual it satisfies two of the most demanding criteria in gdpr. External data sources. Uniquely to a real person. This is how the silverware actually achieves that high bar for true irreversible anonymity i have a question here. If you train a generative model like a jam. On super sensitive data don't those models have a known vulnerability where they can sometimes memorize specific training examples. That could expose real individuals. That is an excellent and a critical question the architecture absolutely anticipates that risk so what's the defense that's why they mandate the use of a dpgm. Degenerative model itself is trained using a differentially private stochastic gradient descent optimizer or dpsd it's a method that works by one clipping the gradients which limits the influence of any single data point during training and two adding noise during the training loop itself. This mathematically ensures the model cannot memorize or inadvertently leaked individual training examples. It neutralizes that known jan vulnerability right at the source that is a very deep level of assurance. Highly processed synthesized data gets promoted to the gold layer it has to pass a rigorous privacy validation. What is that scorecard look like. The scorecard is the technical proof that the anonymization actually worked as intended. Using tools like pie cannon the data is checked against well-established privacy benchmarks. Exactly. Which ensures every record is indistinguishable from at least kale other records in the data set. It also checks for dollar diversity and toe closen. Which measure the diversity of sensitive attributes within those indistinguishable groups and if it fails if the validation scores too low for example if either dollars is less than 5 meaning you have groups of only one or two people sharing characteristics making them highly identifiable the pipeline fails. It triggers an automated retraining of the synthetic model but this time with higher noise parameters until the required level of privacy is mathematically guaranteed and it passes the scorecard okay now let's tackle the multi-jurisdictional compliance challenge head on. Starting with gdpr in. We've established that the silver layers dp in sdg satisfy the requirements for true irreversible anonymity. Yes. And this has a huge legal implication. By moving the data past simple pseudonym and into the state of true quantified anonymity. The gold layer data potentially falls out of the regulatory scope of gdpr entirely. Which provides a legal safe harbor a critical one it allows the research and modeling teams to use the data without the intense transactional compliance burdens that come with handling personal data. Next hep which governs correctional health data. The standard ip ap safe harbor method seems way too destructive for utility. Forcing the removal of features you absolutely need oh it is entirely too destructive for a complex machine learning task like recidivism forecasting. The standard safe harbor method requires removing 18 specific identifiers for example you have to remove all geographical data smaller than a state. If you do that you destroy your ability to analyze spatial risk factors. Socioeconomic status. Community resource access. All of which are critical for accurate. Fair forecast. Lobotomize the model's ability to understand the environment so the architecture bypasses the safe harbor method and ops for the expert determination path instead correct. Expert determination is the alternative legal route under ipay. It allows you to retain data if it qualifies statistical expert can certify that the risk of re-identification is. Very small. And this architecture automates that certification it automates the proof. Buy strictly integrating and enforcing epsilon differential privacy keeping that budget low say epsilon 1.00 which represents a very low loss of privacy the system provides mathematical evidence that the risk is below the required legal threshold to get to keep useful features you get to retain valuable localized features like puma or census tract data. While still having a guaranteed legal defense against privacy claims. The architecture turns a complex legal assessment into an automated quantifiable proof. And finally there are the requirements of the ccpa in california which focuses heavily on functional separation. Does ccpa requires robust technical and procedural safeguards to prevent inadvertent linkage or re-identification. To satisfy this the architecture implements a strong privacy firewall what does that mean in practice. It means physically and logically separating the environments the bronze environment which holds the cryptographic keys used for the initial the keys to unlock the original data exactly. That environment must be completely separated from the gold layer where the final model is trained. And critically those cryptographic keys have to be stored in a highly secured location typically a hardware security module or hsm that is totally inaccessible to anyone or any process in the downstream model training environment so even if the gold layers breach the keys aren't there the key is to dionis the original bronze data are not. This mandatory separation satisfies the ccpa's high bar for technical safeguards and true functional separ. Okay so we've established that the data entering the gold layer is now anonymous it's validated and it's legally compliant across multiple jur. But huge achievement in itself it is. But before it can be used for modeling we have to address the causal filter that move beyond mirror correlation that we discussed. This to me is the most fascinating part because it requires the architecture to intervene in the very science of the feature set. It recognizes that feeding a perfectly private. But correlation filled data set to a model. Will still lead to codified systemic. Rigorous causal discovery phase before any model training happens. If we go back to the p u m a example geographic area right. Strongly with recidivism but it's probably just a proxy for a deeper issues like poverty and over policing. The causal filter is designed to expose pua as a spurious correlation a confounder and remove it. It retains only the features that are truly causally relevant to the outcome how on earth does the system automate such a complex scientific task. It uses something called an automated causal discovery auto cd pipeline. It leverages sophisticated libraries like causal neck. To analyze all the conditional independence in the data and mathematically learn a directed a cyclic graph or a dag is what exactly think of it as a hypothetical map of cause and effect relationships between all the variables in your data set it draws arrows showing which variables seem to directly influence others so once that caused a map is built how does the system decide which features to keep for the model and which to discard. That is the power of the markov blank. The markov blanket of a target variable in our case recidivism is the minimum set of variables you need to perfectly predict that target the minimal set yes. It consists of the target's parents which are its direct causes its children which are its direct effects. And the parents of its. So other variables that share a common effect. Sounds complex but what's the architectural payoff of finding. And most importantly ethical filtering. If a feature is inside the mark off blanket it has an independent causal signal or it mediates a causal path to the target. Crucially anything outside the mark off blanket is statistically independent of the target given the variables inside the blanket so back to our puma example if the influence of poe met the geography is fully explained in mediated by a direct cause that's in the blanket like employment status then poa itself is rendered redundant its excluded from the final future set so you successfully filter out the potentially biased spiritual proxy feature exactly. This ensures the model only learns from what the system has identified as validated causal drivers the causation and social science and criminal justice is highly subjective it's constantly debated. How can we trust an automatically generated causal day we can't not blindly. And that's where scientific rigor comes in. The architecture mandates reputation testing using tools like doei the causal graph that's produced by the auto city pipeline is just a hypothesis and it has to be challenged for its robust. And a key way to do that is by applying a placebo. What on earth is a placebo treatment in this. Okay so you take a feature that the dad claims is a true cause let's say. Participation in rehabilitation program a. You then randomly permute or shuffle that features values across all the record. So you're breaking its real connection to the outcome. You're replacing it with random noise. Then you check if the model still predicts the same causal effect a reduction in recidivism if the model still predicts an effect the causal assumption is invalid because random noise shouldn't cause a defined outcome precisely. This kind of rigorous testing prevents the system from blindly accepting flawed or weak causal assumptions. And given the subjectivity this whole process still requires human in the loop governance to finalize the causal reasoning oh absolutely the architecture incorporates a dashboard where domain experts parole board members social scientists community leaders can review the causal dag that the system proposes and they can edit. They aren't just clicking yes. They can actively approve reject or even modify specific edges the relationships in the graph based on the real world knowledge and established social science principles. And that's all logged every single expert decision is logged. This creates an essential contestable audit trail for all the causal assumptions which is absolutely vital for any kind of legal review or due process okay so once we finally have this. Causley validated. Filtered feature set weekend at last proceed to fairness mitig. Now the data is causally clean but it might still reflect these base rate differences and outcomes due to past systemic factors. So using tools like aif 360 the architecture applies pre-processing techniques like three-way and being a primary example yes and weighing is powerful because it doesn't try to fix the models output after the. It fixes the data distribution before the model even. How does relaying actually work on the records. Calculate specific weights for each record to balance the statistical base rate of recidivism across protected groups say race or gender. So if one group has a higher historical rate. If the baseline recidivism rate is statistically higher for one group which reflects historical bias in the system. Rewind a science lower weights to those records and higher weights to records from the underrepresented group that have similar outcomes the goal is to make the input data distribution. Fair by design exactly so that any downstream model trained on it will inherently minimize discrimination because the bias and the base rates has already been neutralized and the success of this mitigation is measured immediately yes the gold layer calculates standard fairness metrics right away things like demographic parody difference dpd. Which measures the difference in positive outcomes across. An equalized odds difference eod which focuses on the difference in error. So false positives and false negatives. These checks ensure the mitigation was effective before the data is finalized and published as a feature. So this architecture really does transform ethics. From a fuzzy checklist. Into a quantifiable system. You have to prove mathematically. That your system is ethically sound. And that's the whole purpose of the multidimensional score. Correct. The sources proposed evaluating the final outcome not just on performance which is the old way but across four critical dimensions that correspond to the architectural priorities number one is utility. This is measured by the briar score lower is better and for recidivism forecasting your ideally targeting below 0.17. This just assesses the raw pre. Get. Number two is fair. This is measured by fpr parody which is the difference in false positive rates between protect. In a false positive here is a very bad. It's a terrible thing it means incorrectly predicting a high risk for someone who would not have received. That leads to undue custody extended supervision real human harm. The target here is a maximum difference of 0.05 or 5% between groups privacy. This is measured by the tips on risk which is the effective epsilon budget that was consumed. The target is to keep epsilon at or below 1.0 and finally number four. Causal. This is measured by the structural hamming distance or shd. Okay let's pause on shd we talked about the causal deck being a hypothesis. What exactly does the structural hamming distance measure in this. Measures the similarity or you could say the distance between two different causal graphs. The automated dag that was learned by the system and the final approved tag that was established by the human in the loop domain expert. How does it count that. It literally counts the minimum number of edge editions deletions or reversals you need to perform to transform one graph into the other so if the shd is zero it means the machine hypothesis. Perfectly matched the human experts final validated causal map exactly the goal of minimize that distance. A low shd provides quantifiable proof that the automated filtering process aligns with established domain knowledge and judicial reasoning. It proves the future selected are indeed the causally relevant ones that were approved by the experts okay so all of these met. Utility fairness. Privacy causal. They all roll up into the ultimate governance metric. The fair care score this is the composite metric it ranges from 0 to 1.0 and its systematically combines the layer specific scores 68 caspian $60 using configurable. So it moves the ethical assessment out of subjectivity and into a clear audible number know why do the default weights emphasize the silver layer with its weight 2.00 hour at 0.40. That emphasis is very deliberate it's because privacy preservation and the automated causal validation the core functions of the silver layer are the most novel the most legally critical and frankly the most technically challenging aspects of the entire architect. And if your initial causal hypotheses are completely flawed. Any downstream fairness mitigation you do in the gold layer is basically irrelevant it's garbage and garbage out. But the waiting allows for flexibility. If a state department's primary focus was intensely on reducing judicial bias. They might lose the weight of the gold lyrics precisely. They might set the weight for gold wg to serve you to work 50 cents to prioritize fairness over the other metrics. And the system provides clear interpretive guidance the final score of 0.85 or higher indicates robust ethical governance ready for deployment and a low score anything below 0.70 automatically flags a significant risk that requires an architectural review or a full pipeline retraining. So the sources they validate this entire structure using real-world data. Specifically the nij recidivism forecasting challenge data set from georgia. What did the experimental results show about these architectural trade-offs they confirmed the courtesy of the fair care approach. That significant ethical gain comes at a marginal loss of utility. Okay compared to a baseline standard etl pipeline the full fair care architecture achieve massive transformative reductions in bias we're talking to 57% reduction in demographic parody difference. And a 54 percent reduction in equalized odds difference they are it's a huge societal benefit but what was the cost in predictive utility the briars score the cost was by all accounts except. While the original winners of that challenge achieve briar scores around 0.15. The faron private model after going through the full architecture. Mike degrades slightly to a score of about 0.18 and how do the sources interpret that the interpret it very explicitly given that highly accurate high utility models are inherently difficult to build in these complex social domains anyway the marginal loss and predictive power that 03 difference in brier score is overwhelmingly outweighed by the societal benefit of guaranteed privacy and nondiscrimination so the architecture explicitly codifies an ethical mandate. Prioritize minimizing human harm over maximizing statistical cert. That is the ultimate conclusion success is completely redefined. The measure of an ai system in the justice sector is not how accurate the prediction is but how ethical transparent and legally accountable the prediction process remains. Operationalizing. This really rigorous ethical workflow the complex parts of discovery and synthetic data generation. Requires handling big data. Is can't be done on a single laptop let's talk about the underlying engine the spark plus ray hy. This pairing is absolutely critical for achieving what you could call industrial scale at the computing so apache spark is used for the etl heavy lifting in the bronze layer. It handles massive data parallelism schema enforcement managing acid transactions via delta lake. It handles the volume and the reliability requirements. But spark is designed for data parallelism right doing the same simple task like a filter or aggregation. Across billions of records simultaneously. He tends to struggle with highly complex iterative machine learning task. That's exactly it's limitation. The advanced modules in the silver and gold layers. Training these complex generative models for sdg your running causal discovery. Are complex ml workloads that require a different type of scaling. They require task parallelism right and that is where ray is introduced into the. Why is causal discovery a task that sparks specifically struggles with. Cancel discovery algorithms especially constraint based methods like the pc algorithm involve testing. Thousands maybe millions of conditional independence between variables. These operations can have super exponential complexity depending on how many variables. Not at all it's a classic example of what we call the causal. Erase all of this house ray is a general-purpose cluster computing framework that's designed specifically for complex heterogeneous ml workloads. It allows the architecture to spin up thousands of specialized python actors in parallel in each actor does what. Each actor can be dedicated to testing a specific edge or a specific conditional independence in the causal graph. This task parallelism massively accelerates the complex iterative calculations. They use the ray on spark pattern so they share the same infrastructure but they leverage raise specialized strength. Standard big data pipeline. Time frame it makes it operational moving beyond just structured models this architecture is also designed to support the increasing use of retrieval augmented flms. Where you have human case files and unstructured incident nar. This is a crucial future application. For llm agents that might be assisting staff with resource allocation or generating initial case summaries the gold layer prepares the data by integrating vector storage does vectorizing the text data exactly all the unstructured notes the incident evaluations. That text is converted into vector mbed. I'm crucially these embeddings are indexed and tagged with metadata confirming their fairness status their causal validation and their anonymization. So the lm agent is only allowed to retrieve contacts. From sources that have already passed the full fair care ethical validation. Precisely it prevents the llm from inadvertently accessing or synthesizing information from raw risky or unvalidated data sources. It puts ethical guardrails around the sophisticated generative models. This brings us to the future work they propose. For dressing the biggest challenge in high stakes llm use explainability. They call it semantics explainable ai. Because lms are notorious black boxes which is completely unacceptable in a legal context. The standard lom output a synthesized paragraph is insufficient for due process. If an llm agent provides an insight for a risk assessment a lawyer has to be able to trace why and how that conclusion was reached. What's a proposed extension they propose using a knowledge graph kg to provide the essential semantic grounding how does the knowledge graph connect to the irish system. It starts by designing an ontology a formal structured model for the correctional domain. It defines entities like inmate event program location and all of their structured relationships the retrieved context those vector embeddings from the irs system is then explicitly linked to these structured entities in the knowledge graph what's the practical benefit when the llm gives an output. The benefit is traceability. When the llm produces a recommendation or an insight the explanation panel doesn't just show you the text is it. It can trace the contacts back to the structured relationships in the knowledge graph so it can show its work it can explicitly show the path. And program why is associated with high post-release employment rates. It provides a human scrutable semantically grounded justification for the flms reasoning. This makes the ais output transparent audible and contestable which is what's needed to satisfy the highest standard of due process. And finally we should touch on the future potential of federated learning fl. Within this kind of architecture yes federated learning addresses the reality that this kind of correctional data is often highly siloed across different state departments of corrections or docs. They can't or won't share data. Ethel would allow a central refined model to be trained across all these distinct state silos without the sensitive data ever leaving its local secured environment so the algorithm moves but the raw data stays home. That's the principal it offers an even higher tier of privacy protection. It really paves the way for a federated. So states could benefit from a shared much more powerful model without ever having to share their highly sensitive underlying data records. It maximizes both utility and security at the same time. #out. Blueprint for a fundamental shift and what it means to build large-scale ai systems in sensitive domains. The fair kerr lake house reference architecture. Concern handled by a compliance officer with a checklist to automated code that is enforced layer by layer. By the pipeline itself. It's successfully solves that brutal legal and technical conundrum of maximizing utility. While staying compliant with gdpr ipa and ccpa and it does that by using these advanced privacy enhancing technologies like synthetic data generation and differential privacy. It also proves that rigorous scientific methods like causal inference can actually be scaled to handle big data using specialized engines like ray the sources they really urge the industry to move beyond just. Standard continuous integration and continuous delivery cicd for code quality. And to rapidly adopt continuous fairness integration or cfi for ethical quality and the tools are there though i have 360 ray delta lake they are all technologically mature and ready for implementation. The responsibility now lies firmly with the data architects in the engineers to assemble these tools into govern. Accountable system. And this brings us right back to the ultimate question this architecture forces us and you to confront. It's the pursuit of justice not just statistical certainty we saw the evidence reducing systemic bias by over 50% required accepting a marginal reduction in predictive accuracy that means the architecture is explicitly designed to accept that privacy and fairness might require sacrificing a percentage point of statistical utility. So the provocative thought we want to leave you with is this. In high stakes domains that govern human freedom and life outcomes. The calculation of success changes entirely. What is the acceptable trade-off between maximizing model utility and minimizing human harm. At what precise point does chasing that final percentage point of accuracy become morally indefensible if it comes at the cost of codified systemic bias or lost individual privacy it's a calculation that must now be done in code and hopefully with a conscience until next time keep digging deeper. 