\documentclass[conference]{IEEEtran}
\IEEEoverridecommandlockouts
% The preceding line is only needed to identify funding in the first footnote. If that is unneeded, please comment it out.
% \usepackage{cite}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{algorithmic}
\usepackage{graphicx}
\usepackage{textcomp}
\usepackage{xcolor}
\usepackage{booktabs}
\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
    T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}

% block comments
\usepackage{comment}

% bibliography
\usepackage[backend=biber,style=ieee]{biblatex}
\addbibresource{bibfile.bib}

%algorithm
\usepackage{algorithm}
% \usepackage{algpseudocode}

\begin{document}

\title{A Reference Architecture for FAIR and Ethically Governed Data Pipelines in High-Risk AI Domains}
% \title{FAIR-CARE Lakehouse: A Reference Architecture for Ethically Governed Data Pipelines in High-Risk AI Systems}

% \title{A Data Lake Architecture for Fair and Accountable AI in Correctional Systems\\
% \thanks{Identify applicable funding agency here. If none, delete this.}
% }

\begin{comment}
    
% Line break and center authors
\makeatletter
\newcommand{\linebreakand}{%
  \end{@IEEEauthorhalign}
  \hfill\mbox{}\par
  \mbox{}\hfill\begin{@IEEEauthorhalign}
}
\makeatother

\author{
\IEEEauthorblockN{1\textsuperscript{st} Anilson Monteiro}
\IEEEauthorblockA{\textit{University of Beira Interior} \\
Covilhã, Portugal \\
anilson.monteiro@ubi.pt\\
ORCID: 0009-0001-9050-6210}
\and
\IEEEauthorblockN{4\textsuperscript{nd} M. Luqman Jamil}
\IEEEauthorblockA{\textit{University of Beira Interior} \\
Covilhã, Portugal \\
luqman.jamil@ubi.pt\\
ORCID: 0000-0003-3786-0744}
\and
\IEEEauthorblockN{3\textsuperscript{rd} Luis Silva}
\IEEEauthorblockA{\textit{University of Beira Interior} \\
Covilhã, Portugal \\
-----@ubi.pt\\
ORCID: -------------------}
\linebreakand
\IEEEauthorblockN{1\textsuperscript{th} Sebastião Pais}
\IEEEauthorblockA{\textit{University of Beira Interior, NOVA LINCS} \\
Covilhã, Portugal \\
sebastiao@ubi.pt \\
ORCID: 0000-0003-2337-0779}
\and
\IEEEauthorblockN{5\textsuperscript{th} Nuno Pombo}
\IEEEauthorblockA{\textit{University of Beira Interior, Instituto de Telecomunicações} \\
Covilhã, Portugal \\
ngpombo@ubi.pt \\
ORCID: 0000-0001-7797-8849}
}
\end{comment}

\author{
\IEEEauthorblockN{Anonymous Authors}
\IEEEauthorblockA{
Paper submitted for double-blind review.\\
Affiliations withheld to preserve anonymity.}
}

\maketitle

\begin{abstract}
AI systems deployed in high-risk public-sector domains increasingly depend on data pipelines whose architectural properties directly shape fairness, privacy, and accountability. Existing data lakehouse and ETL architectures provide strong guarantees for quality and scalability, yet lack mechanisms to prevent historical bias, privacy leakage, and regulatory non-compliance from propagating into downstream models. To address this gap, we propose the \textit{FAIR-CARE Lakehouse}, a reference architecture that embeds FAIR data principles together with causal validity, differential privacy, and fairness-by-design constraints across the Medallion (Bronze–Silver–Gold) lifecycle. The architecture integrates PII detection, privacy-preserving transformations, causal structure learning, and fairness-aware feature governance into an end-to-end pipeline supported by a measurable \textit{FAIR-CARE Score} that quantifies ethical readiness. We validate the approach using benchmark datasets (COMPAS, Adult, German Credit) and a high-risk case study from the NIJ Recidivism Challenge. Results show that the architecture reduces Equalized Odds disparities by up to 60\% while maintaining competitive predictive performance under strict privacy budgets. These findings demonstrate that ethical guarantees can be enforced as architectural constraints rather than post-hoc interventions. The work contributes a reusable reference architecture, an operational scoring framework, and empirical evidence illustrating how software architects can govern socio-technical risks in continuous AI-enabled systems.
\end{abstract}



\begin{IEEEkeywords}
Lakehouse Architecture, Ethical Data Pipeline, Bias Mitigation,Data Anonymization, Data Governance, Data Provenance
\end{IEEEkeywords}

\begin{comment}
    
\section{Introduction}

% Motivation

% Problem Statement

% Research Gaps / Research Questions
Despite progress in semantic data integration, there are a few
fundamental challenges still unresolved:


% Contributions
To address these limitations, this paper proposes a....

The key contributions of this work are:

% Paper Organisation
The remainder of this paper is structured as follows: 


\section{Related Work}

\subsection{Artificial Intelligence in High-Risk Domains}

Corrections, Judiciary, Healthcare

\subsection{Architectural Frameworks for Trustworthy Artificial Intelligence}

\subsection{FAIR Principles and Ethical data governance}

\subsection{Data Lakehouse Architectures (Medallion Model)}

\subsection{Fairness, Bias, and Causality in Machine Learning}

\subsection{Anonymization Techniques}
k-Anonymity, l-Diversity, t-Closeness, Differential Privacy

\subsection{Gaps in Current Literature}

No architectural integration

No continuous fairness+privacy evaluation

No causal HITL governance
\end{comment}

\section{Introduction}
The growing integration of AI-driven decision-making into public-sector and safety-critical environments has introduced new pressures on software architecture. Traditional architectural concerns, performance, scalability, interoperability, are no longer sufficient when system outputs directly influence individual rights, access to resources, or legal determinations. In domains such as corrections, public safety, and healthcare, the software architect is increasingly responsible for ensuring that data infrastructures uphold principles of fairness, privacy, causal validity, and regulatory compliance. These requirements redefine the architectural role from solely optimizing technical qualities to governing socio-technical risks embedded in the data lifecycle, motivating the need for structured, end-to-end data governance pipelines such as the one depicted in Figure~\ref{fig:arch}. As AI systems become embedded in these sensitive settings, mounting evidence shows that trustworthiness depends not only on accuracy but also on robustness, transparency, accountability, and the mitigation of systemic and historical biases \cite{li2021trustworthy, mitchell2019model}. Algorithmic tools such as COMPAS have demonstrated racial disparities that arise even when protected attributes are excluded from training \cite{angwin2016machine, dressel2018accuracy}. In healthcare, biased datasets can propagate inequities in diagnosis and treatment \cite{plos2022healthcare}.  For instance, 
Obermeyer et al. demonstrated that a widely-used commercial algorithm for managing patient care exhibited significant racial bias by using healthcare costs as a proxy for health needs, resulting in Black patients being systematically undertreated \cite{obermeyer2019dissecting}. Rajkomar et al. emphasize that ensuring fairness in healthcare ML requires attention throughout the entire development lifecycle \cite{rajkomar2018ensuring}. These cases illustrate that failures in data governance manifest as failures in system behavior, reinforcing the need for architectural frameworks that embed ethical safeguards \textit{by design} rather than as post-hoc patches.
\begin{figure*}[t]
    \centering
    \includegraphics[width=\textwidth]{images/arch.png}
    \caption{The FAIR-CARE Lakehouse pipeline architecture, illustrating the end-to-end governance flow across the Bronze, Silver, and Gold layers. Each stage embeds privacy, causal validity, and fairness mechanisms into the data lifecycle.}
    \label{fig:arch}
\end{figure*}

\subsection{Motivation: Architecting for High-Risk AI Systems}
High-risk AI systems are characterized by decisions with direct social, legal, or safety consequences. In such environments, data pipelines become critical architectural assets, as they mediate how information is collected, transformed, and exposed to downstream models. Ensuring the trustworthiness of these systems therefore requires moving beyond conventional performance-oriented ETL pipelines and explicitly managing non-functional requirements (NFRs) such as fairness, privacy preservation, lineage integrity, and auditability \cite{li2021trustworthy, chouldechova2017fair}.

Correctional datasets exemplify the challenges of high-risk governance. Recidivism forecasting models are susceptible to ``data bias,'' where patterns reflect historical policing practices rather than behavioral risk \cite{nij_context}. Privacy risks are equally substantial: combinations of quasi-identifiers such as release dates, geographic zones, and offense categories can re-identify individuals even after direct identifiers are removed \cite{sweeney2002k}. These issues highlight that ethical and regulatory requirements must be operationalized at the architectural level, not delegated to model developers or compliance teams in isolation.

\subsection{Problem Statement}
Conventional Extract--Transform--Load (ETL) and lakehouse architectures are optimized for throughput, schema evolution, and analytical readiness, but they are not designed to enforce ethical constraints throughout the data lifecycle. As a result, they exhibit several limitations:

\begin{itemize}
    \item \textbf{Fragmented Responsibility:} Ethical constraints such as bias mitigation, anonymization, and compliance are distributed across disconnected teams, with no architectural component providing end-to-end governance \cite{veulemans2024detecting}.
    
    \item \textbf{Causal Opacity:} Traditional pipelines treat correlations as sufficient signals for downstream learning, offering no mechanisms to identify or suppress spurious causal paths (e.g., Zip Code~$\rightarrow$ Race~$\rightarrow$ Risk) \cite{smuc2022causal}.
    
    \item \textbf{Regulatory Misalignment:} Standards such as GDPR anonymization, HIPAA Expert Determination, and CCPA functional separation are rarely embedded into pipeline logic; instead, compliance is validated only after processing \cite{gdpr2016}.
\end{itemize}

These limitations point to the need for a unified architectural framework that integrates fairness, privacy, lineage governance, and causal reasoning as first-class architectural citizens. Addressing these gaps requires reconceptualizing the role of the software architect as a steward of data ethics and accountable system behavior.

\subsection{Research Questions}
This work investigates how ethical governance can be embedded structurally into data pipelines for high-risk AI systems. We formulate the following research questions:

\begin{itemize}
    \item \textbf{RQ1:} How can we architect an end-to-end data pipeline that enforces ethical governance, fairness, privacy, and compliance at each stage of the Medallion lakehouse model?
    
    \item \textbf{RQ2:} Which technical components (e.g., causal analysis, differential privacy, synthetic data generation, bias mitigation) are required within each layer to satisfy CARE requirements without causing unacceptable utility loss?
    
    \item \textbf{RQ3:} How can we quantitatively assess the ethical readiness of datasets in high-risk domains by combining FAIR data principles with explicit ethical criteria into a composite, layer-aware metric?
\end{itemize}

\subsection{Contributions}
To address these challenges, this paper introduces the \textbf{FAIR-CARE Lakehouse}, a reference architecture that embeds FAIR data principles alongside the CARE ethical framework (Causality, Anonymity, Regulatory compliance, Ethics) across the Medallion lifecycle. Our contributions are as follows:

\begin{enumerate}
    \item \textbf{Reference Architecture:} We propose an extension of the Medallion Lakehouse (Bronze--Silver--Gold) that incorporates Privacy, Enhancing Technologies (PETs), causal inference mechanisms, and structured governance policies as architectural elements.
    
    \item \textbf{Algorithmic Framework:} We design a multi-stage pipeline that includes PII detection, Differential Privacy (DP), Synthetic Data Generation (SDG), causal structure learning, and human-in-the-loop validation, implemented as modular, reusable architectural components.
    
    \item \textbf{Evaluation Framework:} We introduce the \textbf{FAIR-CARE Score}, a composite, layer-specific metric that evaluates privacy leakage, fairness disparities, lineage completeness, and causal soundness to quantify ethical readiness.
    
    \item \textbf{Regulatory Compliance Mapping:} We demonstrate how architectural parameters (e.g., DP privacy budget $\epsilon$, k-anonymity thresholds, lineage granularity) can be configured to align with GDPR, HIPAA, and CCPA requirements.
    
    \item \textbf{Empirical Validation:} Using benchmark datasets from the NIJ Recidivism Forecasting Challenge \cite{nij_challenge} and widely studied fairness datasets, we illustrate how legacy correctional data can be transformed into a compliant, fair, and analytically valuable asset.
\end{enumerate}

\section{Related Work}

Research on trustworthy AI, privacy-preserving data engineering, and data lakehouse architectures has advanced substantially, yet existing approaches remain siloed and insufficient for architecting high-risk AI systems. Prior work in AI ethics has thoroughly documented how biased datasets, opaque transformations, and ungoverned model pipelines can propagate systemic harms \cite{li2021trustworthy, mitchell2019model}. Analyses of recidivism risk assessment tools such as COMPAS demonstrate that eliminating protected attributes does not prevent disparate outcomes when upstream data transformations embed structural proxies for race or socioeconomic status \cite{angwin2016machine, dressel2018accuracy}. However, these studies focus predominantly on model-level fairness interventions. They offer limited insight into how architectural decisions within data pipelines contribute to bias propagation. As a result, the literature lacks pipeline-level mechanisms for ensuring that fairness, causal validity, and transparency are preserved across the entire data lifecycle.

Regulatory frameworks such as GDPR, HIPAA, and CCPA provide detailed criteria for anonymization, pseudonymization, and functional separation \cite{edpb2014anonymization, hipaa_method, ccpa2018}. Yet engineering practice often implements these requirements through ad-hoc masking rules, retrospective audits, or access-control policies approaches that are brittle, manually intensive, and poorly aligned with continuous data flows in modern architectures. Because compliance checks occur only after data has traversed multiple systems, opportunities for leakage or misuse remain. Existing compliance research thus provides conceptual clarity but lacks architectural abstractions that enforce privacy guarantees at design time rather than as post-hoc corrections.

Work at the intersection of privacy-preserving data engineering and algorithmic fairness provides important building blocks, including Differential Privacy (DP) \cite{dwork2006calibrating}, synthetic data generation techniques \cite{schneider2022investigating}, and fairness metrics such as Equalized Odds and Demographic Parity \cite{hardt2016equality}. Causal frameworks such as Counterfactual Fairness \cite{kusner2017counterfactual} offer principled tools for identifying spurious dependencies. Yet these methods are typically applied in isolation, at the modeling stage, and are rarely integrated into upstream transformation logic. Moreover, pipeline implementations seldom track lineage or enforce governance constraints that ensure these methods are applied consistently and auditable over time. Existing ML pipelines therefore treat ethical safeguards as preprocessing steps, not as continuous architectural guarantees.

Data lakehouse architectures most notably the Medallion pattern (Bronze–Silver–Gold) \cite{databricks_medallion} deliver strong support for schema evolution, scalable ETL/ELT, and analytical readiness. However, their native capabilities stop at data quality and access control; they do not encode fairness constraints, privacy budgets, causal validation, or regulatory mappings into the semantics of pipeline transformations. Audit logs and cataloging systems offer partial governance, but ethical guarantees are not woven into the architectural fabric. As a result, lakehouses provide a promising structural foundation but fall short of supporting high-risk AI workloads requiring provable governance.

The limitations of existing approaches are summarized in Table~\ref{tab:rw_compare}. Across domains, the literature provides conceptual tools (fairness metrics, privacy mechanisms, causal reasoning), but there is no unified architecture that:  
(1) treats fairness and privacy as first-class architectural concerns;  
(2) integrates PETs, causal validation, and fairness filtering directly into Medallion-layer transformations;  
(3) ensures continuous, lineage-aware governance; and  
(4) provides a measurable governance metric for dataset readiness.  
The FAIR-CARE Lakehouse directly addresses these gaps by operationalizing ethical constraints as intrinsic pipeline properties rather than external evaluations.

\begin{table*}[ht]
\centering
\caption{Comparison of Existing Approaches and the FAIR-CARE Lakehouse}
\label{tab:rw_compare}
\begin{tabular}{p{3.6cm} p{3.8cm} p{4.0cm} p{4.2cm}}
\toprule
\textbf{Approach} & \textbf{Strengths in Literature} & \textbf{Limitations for High-Risk AI Systems} & \textbf{How FAIR-CARE Lakehouse Addresses the Gap} \\
\midrule

AI Ethics and Bias Studies &
Identifies bias propagation and fairness failures; provides evaluation metrics \cite{hardt2016equality}. &
Focuses on model-level fixes; no architectural prevention of bias in pipelines. &
Causal validation and fairness filtering embedded at Silver/Gold layers prevent bias before model training. \\

Regulatory Compliance (GDPR, HIPAA, CCPA) &
Clear definitions of anonymization, re-identification, and functional separation \cite{edpb2014anonymization}. &
Compliance enforced via policies or manual audits; not architecturally guaranteed. &
PETs, privacy budgets, and functional separation implemented as structural pipeline constraints. \\

Privacy-Preserving Data Engineering &
Provides DP, SDG, and anonymization techniques \cite{dwork2006calibrating}. &
Techniques applied ad-hoc; no lineage tracking or integration with fairness or causal checks. &
DP-SDG integration with lineage metadata and causal DAG validation within pipeline layers. \\

Causal Inference and Fairness &
Defines causal fairness frameworks and spurious-path detection \cite{kusner2017counterfactual}. &
Implemented at model stage; does not govern upstream transformations. &
Causal DAGs govern feature selection, confounder suppression, and transformation logic. \\

Data Lakehouse Architectures &
Robust schema management and scalable ETL/ELT \cite{databricks_medallion}. &
No ethical governance, privacy enforcement, or fairness auditing across layers. &
Extends Medallion architecture with CARE governance mechanisms and FAIR-CARE scoring. \\

\bottomrule
\end{tabular}
\end{table*}

In adjacent domains such as healthcare, large-scale studies highlight how ungoverned data pipelines can encode and magnify structural inequities. Obermeyer et al.\ \cite{obermeyer2019dissecting} revealed pervasive racial bias in a commercial health risk model affecting over 200 million patients, rooted not in model algorithms but in upstream data proxies. Rajkomar et al.\ \cite{rajkomar2018ensuring} emphasize the need for continuous lifecycle monitoring and stewardship principles parallel to those required for correctional systems. These findings further reinforce that high-risk domains demand architectural, not merely algorithmic, interventions.

Taken together, existing research provides essential conceptual components fairness metrics, differential privacy, causal reasoning, and scalable lakehouse infrastructures, but does not unify them into a governed architecture capable of supporting high-risk AI systems. The FAIR-CARE Lakehouse fills this gap by embedding ethical constraints into the structural layers of the Medallion architecture, enabling continuous, auditable, and legally aligned data governance.


\section{Methodology}

\subsection{Architectural Overview}
We propose the \textbf{FAIR-CARE Lakehouse}, a reference architecture that integrates FAIR data principles with the CARE ethical framework (Causality, Anonymity, Regulatory compliance, Ethics). As illustrated in Figure \ref{fig:arch}, the system extends the standard Medallion pattern (Bronze--Silver--Gold) \cite{databricks_medallion} by embedding privacy-enhancing technologies (PETs) and causal inference engines directly into the transformation logic.

The architecture is implemented using a hybrid distributed computing engine. While Apache Spark is utilized for the ``heavy lifting'' of ETL and schema enforcement in the Bronze layer, we utilize modular Python components in the Silver and Gold layers. This division allows for the integration of specialized libraries required for causal discovery algorithms and synthetic data generation.


\subsection{Layered Design and Transformations}

\subsubsection{Bronze Layer: The Quarantine Zone}
The Bronze layer serves as the immutable system of record. Data enters via Spark Structured Streaming, ensuring ACID compliance through Delta Lake.
\begin{itemize}
    \item \textbf{PII Detection:} Upon ingestion, an automated scanner (utilizing Microsoft Presidio and Regex patterns) scans columns for Personally Identifiable Information (PII) and quasi-identifiers (e.g., Zip Code, Release Date).
    \item \textbf{Pseudonymization:} Direct identifiers are hashed using a salted key stored in a hardware security module (HSM), implementing the ``Functional Separation'' required by CCPA \cite{ccpa2018}.
    \item \textbf{Provenance:} Metadata regarding source, ingestion timestamp, and schema hash are logged to the Unity Catalog.
\end{itemize}

\subsubsection{Silver Layer: The Privacy and Causal Engine}
The Silver layer transforms risky, pseudonymized data into research-ready assets using isolated Python tasks.
\begin{itemize}
    \item \textbf{Differential Privacy (DP):} We implement $\epsilon$-Differential Privacy using the Laplace mechanism to perturb numeric columns. A privacy budget ($\epsilon$) is tracked by the Control Plane. Should the budget be exhausted, the dataset is locked to prevent reconstruction attacks \cite{dwork2006calibrating}.
    \item \textbf{Adaptive Anonymization:} For non-numeric attributes, the system dynamically selects between $k$-anonymity, $l$-diversity, and $t$-closeness based on the sensitivity of the data and the distribution of quasi-identifiers, ensuring granular protection against re-identification.
    \item \textbf{Causal Discovery:} Unlike standard ETL, this layer constructs a Causal Directed Acyclic Graph (DAG) using algorithms such as PC or NOTEARS via the \textit{CausalNex} library \cite{causalnex}. This graph identifies confounders (e.g., Race $\rightarrow$ SES $\rightarrow$ Recidivism) to prevent the modeling of spurious correlations.
\end{itemize}

\subsubsection{Gold Layer: Fairness and Feature Store}
The Gold layer produces the final feature store.
\begin{itemize}
    \item \textbf{Markov Blanket Selection:} Features are filtered based on the causal DAG; only features in the Markov Blanket of the target variable are retained, ensuring causal sufficiency \cite{dowhy_intro}.
    \item \textbf{Bias Mitigation:} We apply pre-processing algorithms from AIF360, such as \textit{Reweighing} or \textit{Disparate Impact Remover}, to correct historical imbalances \cite{aif360_github}.
    \item \textbf{Vector Embedding:} For Retrieval-Augmented Generation (RAG) applications, text fields (e.g., case narratives) are vectorized and stored in Qdrant, tagged with their fairness certification.
\end{itemize}

\subsection{The FAIR-CARE Score}
To quantify the ethical readiness of the data, we define a composite score calculated at runtime. The score is a weighted sum of layer-specific metrics:

\begin{equation}
    \text{Score}_{FC} = w_B S_B + w_S S_S + w_G S_G
\end{equation}

Where the sub-scores are defined as:

\begin{enumerate}
    \item \textbf{Bronze Score ($S_B$):} Measures ingestion quality.
    $$S_B = \frac{1}{3}(\text{Provenance}_{\%} + \text{PII}_{\text{Recall}} + \text{Quality}_{\text{Base}})$$
    
    \item \textbf{Silver Score ($S_S$):} Measures privacy-utility balance.
    $$S_S = \frac{1}{4}(\text{Anon}_{\text{Str}} + \text{Util}_{\text{Ret}} + \text{Causal}_{\text{Valid}} + \text{HITL}_{\text{Appr}})$$
    Here, $\text{Anon}_{\text{Str}}$ is derived from the achieved privacy loss ($1 - \frac{\epsilon}{\epsilon_{max}}$) and $k$-anonymity thresholds.
    
    \item \textbf{Gold Score ($S_G$):} Measures downstream fairness.
    $$S_G = \frac{1}{3}(\text{Fairness}_{\text{Met}} + \text{Feat}_{\text{Qual}} + \text{Util}_{\text{Pred}})$$
    $\text{Fairness}_{\text{Met}}$ aggregates pass/fail rates for Demographic Parity and Equalized Odds.
\end{enumerate}

Algorithm \ref{alg:privacy_transformer} demonstrates the logic for the ``Privacy Transformer,'' a key component in the Silver layer that enforces these constraints.

\begin{algorithm}[h]
\caption{Privacy Preserving Transformation (Silver Layer)}\label{alg:privacy_transformer}
\begin{algorithmic}[1]
\REQUIRE Dataset $D$, Config $\{k, l, t, \epsilon, \text{technique}\}$
\ENSURE Anonymized Dataset $D_{safe}$ satisfying Privacy Constraints
\STATE \textbf{Initialize} AnonymizationEngine with config
\IF{$\text{technique} == \text{differential\_privacy}$}
    \STATE $D_{safe} \leftarrow$ \textbf{ApplyLaplaceMechanism}($D$, $\epsilon$)
\ELSIF{$\text{technique} == \text{k\_anonymity}$}
    \STATE $D_{safe} \leftarrow$ \textbf{GeneralizeAndSuppress}($D$, $k$)
\ELSIF{$\text{technique} == \text{l\_diversity}$}
    \STATE $D_{safe} \leftarrow$ \textbf{EnsureDiverseSensitive}($D$, $k$, $l$)
\ELSIF{$\text{technique} == \text{t\_closeness}$}
    \STATE $D_{safe} \leftarrow$ \textbf{EnsureDistributionCloseness}($D$, $k$, $t$)
\ENDIF
\STATE \textbf{Compute} $S_S$ (Silver Score)
\STATE \textbf{Return} $D_{safe}$
\end{algorithmic}
\end{algorithm}


% \section{Experimental Evaluation}

% \subsection{Datasets}
% We validate the architecture using four high-dimensional benchmark datasets widely used in fairness research \cite{li2021trustworthy, nij_challenge}:
% \begin{itemize}
%     \item \textbf{NIJ Recidivism Challenge:} A rich correctional dataset containing 25,000+ records with sensitive attributes (supervision level, drug tests) and recidivism outcomes \cite{nij_context}.
%     \item \textbf{COMPAS:} The canonical dataset for analyzing racial bias in risk assessment \cite{angwin2016machine}.
%     \item \textbf{UCI Adult \& German Credit:} Standard benchmarks for income and credit scoring disparity.
% \end{itemize}

% \subsection{Experimental Setup}
% Experiments were conducted on a cluster running \textbf{Ray on Spark} \cite{ray_on_spark} with 4 worker nodes (16 vCPUs each). The pipeline was configured with strict GDPR-aligned parameters: Privacy Budget $\epsilon < 1.0$ and $k$-anonymity threshold $k=5$. We utilized \textit{ARX} for privacy metrics, \textit{DoWhy} for causal refutation, and \textit{AIF360} for bias mitigation.

% We compare the \textbf{FAIR-CARE Pipeline} against two baselines:
% \begin{enumerate}
%     \item \textbf{Baseline A (Naive ETL):} Standard pseudonymization (hashing IDs) with no statistical anonymization or causal filtering.
%     \item \textbf{Baseline B (Masking):} A pipeline applying static data masking (suppression) without synthetic generation.
% \end{enumerate}

% \subsection{Results}

% \subsubsection{Fairness vs. Utility Trade-off}
% Table \ref{tab:results} summarizes the performance on the NIJ dataset. The FAIR-CARE pipeline, utilizing Causal Discovery to remove spurious confounders (e.g., zip code proxies), reduced the \textit{Equalized Odds Difference} (EOD) by 60\% compared to Baseline A. While synthetic generation introduced a marginal drop in model accuracy (AUC decreased by 0.03), the privacy guarantee ($\epsilon=0.8$) prevents re-identification, satisfying the ``Expert Determination'' standard of HIPAA.

% \begin{table}[htbp]
% \caption{Comparison of Fairness and Utility on NIJ Dataset}
% \begin{center}
% \begin{tabular}{|l|c|c|c|c|}
% \hline
% \textbf{Pipeline} & \textbf{AUC} $\uparrow$ & \textbf{EOD} $\downarrow$ & \textbf{Priv. Risk} $\downarrow$ & \textbf{Score}_{FC} \\
% \hline
% Baseline A (Naive) & \textbf{0.74} & 0.18 & High (Linkable) & 0.45 \\
% Baseline B (Mask) & 0.61 & 0.12 & Low & 0.62 \\
% \textbf{FAIR-CARE} & 0.71 & \textbf{0.07} & \textbf{Provable ($\epsilon$)} & \textbf{0.88} \\
% \hline
% \end{tabular}
% \label{tab:results}
% \end{center}
% \end{table}

% \subsubsection{Causal Validity}
% Using the \textit{DoWhy} refutation test, we validated the causal graphs generated in the Silver Layer. The inclusion of a causal filter rejected 14\% of features that were highly correlated with the target but statistically determined to be spurious (non-causal) proxies for race. This structurally prevents the model from learning bias, a capability absent in standard feature selection methods.
\subsection{Architectural Assumptions and Quality Attribute Trade-offs}
The FAIR-CARE Lakehouse architecture is grounded on several architectural assumptions that guide its design choices and influence its quality attribute trade-offs. First, the architecture assumes that privacy, fairness, and utility cannot be simultaneously maximized; instead, they form a three-way trade-off space analogous to the classical ``Security–Performance'' tension in software architecture. By introducing Differential Privacy, causal filtering, and synthetic data generation in the Silver layer, the system explicitly prioritizes privacy and fairness over raw predictive performance. This trade-off reflects the requirements of high-risk domains, where ethical and legal constraints override marginal accuracy gains.

From an architectural evaluation perspective, the pipeline can be analyzed using scenario-based reasoning inspired by ATAM and QAW-lite. Key scenarios include: (i) \textit{privacy degradation under repeated queries}, (ii) \textit{fairness drift when upstream attributes shift over time}, and (iii) \textit{utility losses when stronger anonymity parameters are applied}. These scenarios inform the placement of Privacy Budgets, Causal DAG validators, and lineage metadata within the pipeline. Although a full ATAM evaluation is beyond the scope of this paper, the architecture incorporates the essential rationale of aligning design decisions with quality attribute priorities.

A further assumption underpinning the architecture is the availability of human expertise to validate causal structures. While the Silver layer provides automated causal discovery, domain experts (e.g., criminologists, legal scholars, or social scientists) must review and refine the discovered DAGs to ensure that critical causal relationships are preserved while spurious or ethically inappropriate paths are removed. This Human-in-the-Loop (HITL) component is not merely a procedural step but an architectural mechanism: the pipeline enforces expert validation checkpoints before data is promoted from the Silver to the Gold layer. This requirement introduces additional operational cost but improves causal validity and regulatory defensibility.

Collectively, these assumptions and trade-offs clarify the architectural role of the FAIR-CARE Lakehouse: it is not a pipeline optimized for maximum predictive accuracy, but a governed data architecture designed to operationalize ethical, legal, and causal constraints as first-class architectural concerns.

\section{Experimental Evaluation}

\subsection{Datasets}
We validate the FAIR-CARE Lakehouse architecture using four widely studied benchmark datasets in fairness and high-risk decision-making research \cite{li2021trustworthy, nij_challenge}. These datasets span correctional forecasting, credit scoring, and income prediction, domains where fairness, privacy, and causal integrity are critical:
\begin{itemize}
    \item \textbf{NIJ Recidivism Challenge:} A correctional dataset containing over 25{,}000 records with sensitive behavioral and supervision attributes, including ground-truth recidivism outcomes \cite{nij_context}.
    \item \textbf{COMPAS:} A canonical benchmark for assessing racial disparities in algorithmic risk assessment \cite{angwin2016machine}.
    \item \textbf{UCI Adult \& German Credit:} Standard fairness benchmarks frequently used to study socioeconomic bias in income and credit scoring.
\end{itemize}

These datasets provide a representative evaluation across multiple high-risk application contexts. Their characteristics are summarized in Table \ref{tab:dataset_characteristics}.

Table~\ref{tab:dataset_characteristics} summarizes their key properties, 
highlighting the demographic imbalances and class skew that motivate our 
fairness-preserving architecture.

\begin{table}[htbp]
\centering
\caption{Benchmark Dataset Characteristics and Bias Indicators}
\label{tab:dataset_characteristics}
\small
\begin{tabular}{lrccp{2.5cm}}
\hline
\textbf{Dataset} & \textbf{Size} & \textbf{Pos.} & \textbf{Sens. Attr.} & \textbf{Primary Bias} \\
\hline
NIJ Recidivism & 25,690 & 30\% & Race, Gender & Gender skew (88\% Male), Racial imbalance (57\% Black) \\
\hline
COMPAS & 7,214 & 45\% & Race, Sex & Racial disparity (52\% African-American, 34\% Caucasian) \\
\hline
Adult Census & 32,561 & 24\% & Race, Sex & Race-income correlation (85\% White, skewed toward <=50K) \\
\hline
German Credit & 1,000 & 30\% & Age, Sex & Small sample, age skew (mean=35, right-tailed), 69\% Male \\
\hline
\end{tabular}
\end{table}

These datasets exhibit varying degrees of class imbalance (24-45\% positive class) and demographic skew. The NIJ and COMPAS datasets originate from actual correctional risk assessment systems, making them particularly relevant for validating our architecture's real-world applicability. Adult Census and German Credit provide cross-domain validation in financial decision-making contexts, where similar fairness concerns arise. Figure \ref{fig:eda} illustrates the demographic distributions and imbalances across the four benchmark datasets.

\begin{figure}[t]
    \centering
    \includegraphics[width=\columnwidth]{images/fig_demographic.png}
    \caption{Demographic characteristics and Class Imbalance across benchmark datasets. (a) NIJ Recidivism shows extreme gender imbalance. (b) COMPAS exhibits racial disparity. (c) Adult Census shows correlation between race and income. (d) Class imbalance varies significantly across datasets.}
    \label{fig:eda}
\end{figure}

\subsection{Experimental Setup}

Experiments were executed on a Spark-based cluster configured with four worker nodes (16 vCPUs each). Spark handles the ingestion and Bronze-layer transformations, while the Silver and Gold layer components for causal discovery, DP-SGD synthetic data generation, and privacy verification are implemented as modular Python workflows orchestrated within the Spark-driven pipeline, without deploying the Ray-based execution model that is proposed in the reference architecture for future large-scale deployments.

To align with regulatory standards, the pipeline was configured with strict parameters:
\begin{itemize}
    \item Privacy Budget: $\epsilon < 1.0$ (GDPR-aligned threshold),
    \item Minimum anonymity guarantee: $k=5$,
    \item Anonymization: Adaptive application of $k$-anonymity, $l$-diversity, and $t$-closeness,
    \item Causal refutation: Performed using \textit{DoWhy},
    \item Bias analysis: Conducted using \textit{AIF360},
    \item Privacy metrics: Computed with \textit{ARX}.
\end{itemize}

We evaluate the FAIR-CARE pipeline relative to two baselines representing common industry practices:
\begin{enumerate}
    \item \textbf{Baseline A (Naive ETL):} Standard pseudonymization using hashed identifiers, without statistical anonymization, causal filtering, or fairness enforcement.
    \item \textbf{Baseline B (Masking):} Static suppression-based masking applied prior to modeling, without synthetic generation or causal validation.
\end{enumerate}

These baselines reflect the typical configurations found in production ETL pipelines and allow us to assess how architectural governance mechanisms influence privacy, fairness, and utility.


\begin{comment}
    
\subsection{Benchmark Comparisons}




\section{Results and Discussion}

Evaluation Framework

Layer-Specific Metrics

Weighted Score Function

Visualizations

Legal and Ethical Analysis

Discussion



\section{Limitations}




\section{Conclusion and Future Work}
\label{sec:conclusion-future-work}
Summary of Contributions

Future works
\cite{turing1950computing}
\end{comment}


\section{Results and Discussion}

\subsection{Experimental Outcomes}
Our evaluation examines how the FAIR-CARE Lakehouse balances the three pillars of the CARE framework—\textit{Privacy}, \textit{Fairness}, and \textit{Utility}. Privacy is quantified through the achieved Differential Privacy budget ($\epsilon$) and $k$-anonymity thresholds; fairness is assessed using Equalized Odds Difference (EOD) and Demographic Parity Difference (DPD); and utility is measured through AUC. Together, these metrics enable holistic assessment of whether the architecture supports ethically governed AI in high-risk settings.


\subsubsection{Benchmark Comparisons Across Fairness Datasets}
We applied the FAIR-CARE pipeline to three widely used fairness datasets: COMPAS, Adult Census, and German Credit. Across all benchmarks, the architecture substantially reduced fairness disparities relative to a naive ETL baseline.

Table~\ref{tab:ablation} presents an ablation study on the COMPAS dataset. The best FAIR‑CARE configurations (Configs A and B) reduce EOD from 0.35 to 0.25 and DPD from 0.28 to 0.20 compared to the baseline, while maintaining utility above 0.85. This pattern shows that introducing $k$-anonymity and differentially private synthetic data generation mitigates disparities at a modest accuracy cost (from 1.00 down to 0.87–0.94), a trade-off aligned with the safeguards expected in high-risk deployments. This aligns with prior findings that Differential Privacy and Synthetic Data Generation can mitigate historical biases by smoothing extreme distributions \cite{veulemans2024detecting}. The utility remains high (0.87–0.94), representing a modest accuracy trade-off consistent with ethical safeguards required in high-risk deployments.

\begin{figure}[t]
    \centering
    \includegraphics[width=\columnwidth]{images/fig3_benchmark_datasets.png}
    \caption{Multi-dataset benchmarking of Bronze (SB), Silver (SS), Gold (SG), and overall FAIR-CARE Scores across Adult, COMPAS, German Credit, and NIJ datasets. Higher bars indicate greater ethical readiness at each layer.}
    \label{fig:benchmark-layer-scores}
\end{figure}

Figure~\ref{fig:benchmark-layer-scores} summarizes layer-wise and composite FAIR-CARE Scores across all four datasets, showing that the architecture consistently improves Bronze, Silver, and Gold governance while keeping the overall FAIR-CARE Score in a high-readiness regime.


\begin{table}[htbp]
\caption{Ablation Study: Impact of FAIR-CARE Layers on Adult Dataset}
\begin{center}
\begin{tabular}{|l|c|c|c|c|}
\hline
\textbf{Configuration} & \textbf{Score}$_{FC}$ & \textbf{SS} (Silver) & \textbf{Privacy} & \textbf{Utility} \\
\hline
Baseline (No CARE) & 0.72 & 1.00 & Low & 1.00 \\
Config A ($k$-anon) & 0.74 & 0.94 & Med & 0.82 \\
Config B (Diff. Priv) & 0.78 & 1.00 & High & 1.00 \\
Config C (Causal) & 0.74 & 1.00 & Med & 1.00 \\
\hline
\multicolumn{5}{l}{\scriptsize scores from exp1.csv evaluation.}\\
\end{tabular}
\label{tab:ablation}
\end{center}
\end{table}

The impact of these configurations on the composite FAIR-CARE Score is visualized in Figure \ref{fig:ablation}. While individual fairness metrics improve, the overall score balances this against utility loss.

\begin{figure}[t]
    \centering
    \includegraphics[width=\columnwidth]{images/fig1_ablation_faircare.png}
    \caption{Ablation Study: FAIR-CARE Score by Configuration (Adult Dataset). The architecture demonstrates a progression towards robust governance (approaching 0.80) as privacy and fairness modules are activated.}
    \label{fig:ablation}
\end{figure}

Figure~\ref{fig:eda} illustrates the demographic imbalances present in the benchmark datasets. The gender skew (67-88\% male across datasets) and racial composition variations (from 52\% African-American in COMPAS to 85\% White in Adult) demonstrate why naive ETL pipelines fail: models trained on these distributions will inherently encode historical biases~\cite{obermeyer2019dissecting, dressel2018accuracy}. 
The FAIR-CARE architecture addresses this through three mechanisms: (1) PII detection and pseudonymization in the Bronze Layer, (2) causal filtering to remove demographic proxies in the Silver Layer, and (3) reweighing and bias mitigation in the Gold Layer.

\subsubsection{Causal Validity Across Pipelines}
As part of the Silver Layer processing, we examined the impact of causal filtering using DoWhy refutation tests. The causal discovery stage explicitly validated the directionality of relationships between sensitive attributes and outcomes. This capability allows the pipeline to flag potential spurious correlations (e.g., ZIP codes acting as proxies for race) that standard ETL processes might blindly ingest. The inclusion of causal validation contributes meaningfully to the composite FAIR-CARE score by penalizing models that rely on unverified assumptions.



\subsection{Case Study: The NIJ Recidivism Challenge}
To assess the architecture in a high-risk, real-world context, we evaluated the NIJ Recidivism Challenge dataset \cite{nij_challenge}, utilizing Logistic Regression and ROC AUC as the primary utility metric:

\begin{itemize}
    \item \textbf{Baseline (Legacy):} Direct modeling on raw data yielded a high AUC of 0.92, indicating strong predictive signal, but presented substantial privacy leakage and high fairness disparity.
    \item \textbf{Silver Layer (Privacy):} Applying the Privacy Preservation module with $\epsilon = 1.0$ maintained a robust AUC of 0.91, showing that utility can be almost fully preserved even under strict privacy constraints.
    \item \textbf{Gold Layer (Fairness):} After bias mitigation via AIF360 Reweighing, we achieved a significant reduction in Statistical Parity Difference, demonstrating that the architecture can actively correct historical biases with minimal loss in predictive utility (Final AUC $\approx$ 0.91).
\end{itemize}

Although the FAIR-CARE pipeline does not maximize predictive accuracy, it produces an \textit{ethically robust} model with demonstrable protections against discriminatory or privacy-violating behavior, as shown in Figure~\ref{fig:nij_case_study}, an essential property for correctional applications.

\begin{figure}[t]
    \centering
    \includegraphics[width=\columnwidth]{images/fig_nij_case_study.png}
    \caption{Case Study Results on NIJ Recidivism Dataset. The chart illustrates the trade-off between predictive utility (AUC) and fairness disparity. While the Baseline model achieves high AUC (0.92), it suffers from high disparity (EOD $\approx$ 0.18). The Gold Layer sacrifices some predictive power (AUC $\approx$ 0.91) to achieve a fairer model (EOD $\approx$ 0.07).}
    \label{fig:nij_case_study}
\end{figure}

\subsection{FAIR-CARE Score Analysis}
The FAIR-CARE Score consolidates ethical and technical metrics across the Medallion layers. As shown in Table~\ref{tab:ablation}, the naive baseline scores lowest (0.72) due to the absence of anonymization and causal safeguards. The full FAIR-CARE configuration (Config B) achieves the highest score of 0.78, approaching the 0.80 threshold indicative of ``Robust Governance.'' This demonstrates that ethical readiness is not a byproduct of isolated privacy or fairness techniques, but emerges from coordinated architectural constraints across Bronze, Silver, and Gold layers.

\begin{figure}[t]
    \centering
    \includegraphics[width=\columnwidth]{images/fig4_benchmark_techniques.png}
    \caption{FAIR-CARE Score by privacy technique (left) and corresponding utility--privacy trade-off (right). Differential Privacy (dp) attains the highest FAIR-CARE Score (0.685) with the best utility retention, but also exhibits the highest residual privacy risk, whereas $k$-anonymity, $l$-diversity, and $t$-closeness achieve slightly lower scores (0.663--0.674) with comparable utility and reduced estimated privacy risk.}
    \label{fig:benchmark-techniques}
\end{figure}


Figure~\ref{fig:benchmark-techniques} compares FAIR-CARE Scores and utility--privacy trade-offs across four privacy techniques, showing that Differential Privacy achieves the highest composite score and utility retention but at the cost of higher residual privacy risk, while classical anonymization methods offer marginally lower FAIR-CARE Scores with similar utility and lower estimated privacy exposure.



\subsection{Legal and Ethical Compliance}
We now evaluate the architectural implications in the context of regulatory frameworks, addressing \textbf{RQ3}:

\begin{itemize}
    \item \textbf{GDPR:} Differential Privacy ($\epsilon \le 1.0$) directly supports the GDPR definition of ``anonymization'' by preventing singling out, linkability, and inference \cite{edpb2014anonymization}.
    \item \textbf{HIPAA:} The Silver Layer implements the ``Expert Determination'' pathway through DP mechanisms and statistical disclosure control, ensuring that residual re-identification risk is ``very small'' \cite{hipaa_method}.
    \item \textbf{CCPA/CPRA:} Functional separation between the Bronze (identifier keys) and Gold (analytic features) layers ensures that downstream consumers cannot reconstruct identity, satisfying statutory separation requirements.
\end{itemize}

Collectively, these results demonstrate that the FAIR-CARE Lakehouse not only improves fairness and privacy performance but also provides a principled architectural mechanism for satisfying regulatory mandates in high-risk AI systems.


\section{Limitations}
Despite these advances, several limitations remain:
\begin{enumerate}
    \item \textbf{Causal Reliance on Experts:} The Causal Graph construction in the Silver Layer relies on domain expert elicitation. While we use algorithmic validation to aid this, fully automated causal discovery remains an open research problem. Our current implementation focuses on validation rather than automated feature suppression.
    \item \textbf{The "Price of Fairness":} As observed in the NIJ experiment, strict fairness constraints led to a performance drop. In scenarios where accuracy is paramount (e.g., violent crime prediction), this trade-off requires careful policy consideration beyond software architecture.
    \item \textbf{Scope:} Our evaluation focused on tabular data. Unstructured modalities (video, audio) require different privacy mechanisms (e.g., deepfakes for anonymization) not covered here.
\end{enumerate}

\subsection*{Artifact Availability}
To support reproducibility, we provide an anonymized code repository with the full FAIR-CARE pipeline implementation and experiment scripts at the following URL (anonymized for review): \url{https://anonymous.4open.science/r/XXXX}. 


\section{Threats to Validity}

Despite the structured design of the FAIR-CARE Lakehouse and its alignment with regulatory, causal, and ethical principles, several threats to validity must be acknowledged. These factors affect the interpretation, generalizability, and robustness of the study's findings.

\subsection{Internal Validity}
Internal validity concerns whether the observed improvements can be reliably attributed to the architecture. 
A first threat is \textbf{causal graph correctness}: the causal graph construction in the Silver Layer depends on domain expert elicitation and algorithmic discovery (e.g., PC, NOTEARS), and these algorithms are sensitive to hyperparameter choices and may be influenced by unobserved confounders, so flawed structures can lead to ineffective or misleading feature filtering in the Gold Layer. 
To mitigate this, we relied on refutation tests (e.g., placebo treatments) via \textit{DoWhy}, but automated causal discovery and validation remain open research challenges. 
A second threat is \textbf{sensitivity to privacy parameters}: the Differential Privacy mechanisms (e.g., Laplace noise) introduce stochasticity, so small deviations in the privacy budget ($\epsilon$) or clipping thresholds can yield different utility–fairness trade-offs, and although we fixed random seeds for reproducibility, single-run results may still vary slightly.

\subsection{External Validity}
External validity pertains to the generalizability of our findings beyond the datasets and contexts studied. 
Our evaluation focuses exclusively on structured tabular datasets, which means the behavior of the architecture for unstructured modalities such as video or free-text narratives remains unknown and would require distinct privacy mechanisms (e.g., redaction, blurring) that are not yet implemented in the Silver Layer. 
Moreover, the datasets used (COMPAS, NIJ, US Census–derived Adult, German Credit) originate from U.S. or closely related socio-legal contexts, so bias patterns and regulatory requirements may differ significantly in international domains (e.g., EU healthcare or social services), and the specific causal structures and fairness–utility trade-offs observed here may not transfer directly.

\subsection{External Validity}
External validity pertains to the generalizability of our findings beyond the specific datasets and settings considered. Our evaluation focused exclusively on structured tabular datasets, which are common in correctional and financial domains, but do not cover unstructured modalities such as video or free-text narratives that require distinct privacy mechanisms (e.g., redaction, blurring, or text anonymization) not currently implemented in our Silver Layer. Moreover, the datasets used (COMPAS, NIJ, US Census–based Adult) originate from U.S. socio-legal contexts. Bias patterns, sensitive attributes, and regulatory requirements may differ significantly in international domains—for example, EU healthcare or social services—so the specific causal structures and fairness–utility trade-offs observed here may not transfer directly across jurisdictions.

\subsection{Construct Validity}
Construct validity addresses whether our metrics accurately reflect the ethical constructs under study. We assessed fairness using group-based metrics, specifically Demographic Parity Difference (DPD) and Equalized Odds Difference (EOD), which are standard but do not capture individual fairness, procedural fairness, or long-term societal impact. Similarly, we operationalized \textit{utility} primarily as AUC, which does not reflect calibration, interpretability, or downstream decision impact that may be critical in high-risk domains. In addition, the FAIR-CARE Score is a composite metric whose weighting scheme ($w_S, w_G$) is architecturally defined. While this weighting is meaningful for comparing configurations within our study, different stakeholder priorities or regulatory interpretations might require alternative weightings that, in turn, could lead to different conclusions about the ethical readiness of a given configuration.

\subsection{Conclusion Validity}
Conclusion validity relates to the statistical strength of our claims. Our quantitative evaluation is based on a limited number of benchmark datasets ($N=4$), and although we observed consistent trends, some paired comparisons lacked statistical significance at the conventional $p < 0.05$ threshold, limiting the strength of inferential claims. Furthermore, the fairness gains demonstrated on static benchmarks may differ in production environments subject to distributional shift (data drift), irregular update cycles, or adversarial behavior. As a result, the real-world robustness of the FAIR-CARE Lakehouse remains to be validated in longitudinal deployments, and future work should incorporate more extensive repetitions, larger and more diverse datasets, and robustness checks under realistic operational conditions.

\section{Conclusion and Future Work}
This paper presented the FAIR-CARE Lakehouse, a reference architecture that unifies FAIR data management principles with ethical governance mechanisms tailored for high-risk AI systems. By extending the Medallion model with privacy-preserving technologies, causal inference components, and lineage-aware enforcement policies, the architecture reframes the data pipeline as a socio-technical system in which fairness, privacy, and regulatory compliance are first-class architectural concerns. In addressing our research questions, we demonstrated that ethical governance can be enforced end-to-end through architectural mechanisms embedded in each stage of the pipeline (RQ1). The combination of differential privacy, synthetic data generation, causal DAG validation, and fairness-aware feature selection forms a technically grounded toolkit that enables enforcement of CARE constraints while maintaining competitive utility (RQ2). Furthermore, we introduced the FAIR-CARE Score as a composite metric that operationalizes ethical readiness and provides a measurable basis for evaluating datasets within high-risk domains (RQ3). These contributions show that architectural design, rather than downstream model adjustments, is the appropriate locus for embedding trustworthy AI principles. Overall, the FAIR-CARE Lakehouse advances the role of software architecture in AI governance by providing a structured, enforceable, and measurable approach to ethical data pipeline design. The architecture offers a foundation upon which future continuous-governance and adaptive compliance mechanisms can be built, supporting the long-term evolution of responsible AI infrastructures.

\subsection{Future Work}

An important extension of the FAIR-CARE architecture is its integration with \textbf{Federated Learning (FL)} to support distributed, privacy-preserving model development across institutional boundaries. Correctional data is typically fragmented across jurisdictions, each governed by distinct legal, operational, and infrastructural constraints. Centralizing such data is often infeasible due to regulatory limitations (e.g., GDPR data minimization) and trust concerns among agencies. Incorporating FL frameworks such as \textit{NVFlare}, \textit{Flower}, or \textit{Ray Fed} would enable collaborative training without raw-data movement, thereby elevating the architecture’s privacy-by-design guarantees. From an architectural perspective, FL introduces new dimensions of continuous governance. Privacy budgets must be managed across federated rounds, fairness must be monitored for participating nodes, and causal structures may diverge across data silos. Future work will explore how the FAIR-CARE Score can be extended to a \textit{federated governance score}, reflecting heterogeneity, drift, and cross-site ethical compliance. Automated governance mechanisms such as node-level anomaly detection, fairness-drift monitoring, and DP-aware aggregation are necessary to maintain ethical guarantees under asynchronous and adversarial training conditions. Integrating FL with the FAIR-CARE Lakehouse, therefore, represents not only a technical enhancement but also a natural step toward \textit{continuous architecting} in high-risk AI ecosystems, where data sources evolve independently, and governance must adapt dynamically.


\begin{comment}
\section*{Acknowledgment}
This work is supported by UIDB/04516/2020 of NOVA Laboratory for Computer Science and Informatics (NOVA LINCS), funded by FCT.IP. It also receives funding from the COMPETE2030 program under the Horus 360 iOMS NextGen project (COMPETE2030-FEDER-01316200).
\end{comment}

\printbibliography

\end{document}
