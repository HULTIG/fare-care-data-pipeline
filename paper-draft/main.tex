\documentclass[conference]{IEEEtran}
\IEEEoverridecommandlockouts
% The preceding line is only needed to identify funding in the first footnote. If that is unneeded, please comment it out.
% \usepackage{cite}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{algorithmic}
\usepackage{graphicx}
\usepackage{textcomp}
\usepackage{xcolor}
\usepackage{booktabs}
\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
    T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}

% block comments
\usepackage{comment}

% bibliography
\usepackage[backend=biber,style=ieee]{biblatex}
\addbibresource{bibfile.bib}

%algorithm
\usepackage{algorithm}
% \usepackage{algpseudocode}

\begin{document}

\title{A Reference Architecture for FAIR and Ethically Governed Data Pipelines in High-Risk AI Domains}
% \title{FAIR-CARE Lakehouse: A Reference Architecture for Ethically Governed Data Pipelines in High-Risk AI Systems}

% \title{A Data Lake Architecture for Fair and Accountable AI in Correctional Systems\\
% \thanks{Identify applicable funding agency here. If none, delete this.}
% }

\begin{comment}
    
% Line break and center authors
\makeatletter
\newcommand{\linebreakand}{%
  \end{@IEEEauthorhalign}
  \hfill\mbox{}\par
  \mbox{}\hfill\begin{@IEEEauthorhalign}
}
\makeatother

\author{
\IEEEauthorblockN{1\textsuperscript{st} Anilson Monteiro}
\IEEEauthorblockA{\textit{University of Beira Interior} \\
Covilhã, Portugal \\
anilson.monteiro@ubi.pt\\
ORCID: 0009-0001-9050-6210}
\and
\IEEEauthorblockN{4\textsuperscript{nd} M. Luqman Jamil}
\IEEEauthorblockA{\textit{University of Beira Interior} \\
Covilhã, Portugal \\
luqman.jamil@ubi.pt\\
ORCID: 0000-0003-3786-0744}
\and
\IEEEauthorblockN{3\textsuperscript{rd} Luis Silva}
\IEEEauthorblockA{\textit{University of Beira Interior} \\
Covilhã, Portugal \\
-----@ubi.pt\\
ORCID: -------------------}
\linebreakand
\IEEEauthorblockN{1\textsuperscript{th} Sebastião Pais}
\IEEEauthorblockA{\textit{University of Beira Interior, NOVA LINCS} \\
Covilhã, Portugal \\
sebastiao@ubi.pt \\
ORCID: 0000-0003-2337-0779}
\and
\IEEEauthorblockN{5\textsuperscript{th} Nuno Pombo}
\IEEEauthorblockA{\textit{University of Beira Interior, Instituto de Telecomunicações} \\
Covilhã, Portugal \\
ngpombo@ubi.pt \\
ORCID: 0000-0001-7797-8849}
}
\end{comment}

\author{
\IEEEauthorblockN{Anonymous Authors}
\IEEEauthorblockA{
Paper submitted for double-blind review.\\
Affiliations withheld to preserve anonymity.}
}

\maketitle

\begin{abstract}
The \textit{FAIR-CARE Lakehouse} is introduced as a reference architecture for ethically governed data pipelines in high-risk AI systems. The increasing growth of data-driven decision-making and its entry into sensitive areas such as corrections, public safety, and healthcare should require software architects to design infrastructures that ensure not only scalability and data quality, but also fairness, privacy, causal integrity, and regulatory compliance.Traditional ETL and lakehouse pipelines remain limited in their ability to address structural bias, preserve anonymization guarantees, or embed governance mechanisms throughout the data lifecycle. 
Our architecture integrates FAIR data principles with the CARE framework, encompassing Causality, Anonymity, Regulatory Compliance, and Ethics, across the Medallion layers of a lakehouse pipeline.The design includes privacy considerations, improved transformations, causal modeling, schema management, vectorization components, and human supervision in the loop to support downstream tasks such as recidivism prediction and LLM agents enhanced with retrieval. We also propose a layer-specific evaluation metric that measures ethical readiness, called the “FAIR-CARE Score,” which is calculated by assessing privacy loss, fairness inequalities, lineage completeness, and causality validity. The approach is validated using widely studied datasets in fairness research, including COMPAS, Adult Census, German Credit, and the NIJ Recidivism Forecasting Challenge, demonstrating that ethical guarantees can be embedded into data pipelines while retaining sufficient analytical utility. This work provides software architects with a structured and actionable blueprint for designing governed data infrastructures capable of supporting trustworthy AI in high-risk environments.
\end{abstract}



\begin{IEEEkeywords}
Lakehouse Architeture, Ethical Data Pipeline, Bias Mitigation,Data Anonymization, Data Governance, Data Provenance
\end{IEEEkeywords}

\begin{comment}
    
\section{Introduction}

% Motivation

% Problem Statement

% Research Gaps / Research Questions
Despite progress in semantic data integration, there are a few
fundamental challenges still unresolved:


% Contributions
To address these limitations, this paper proposes a....

The key contributions of this work are:

% Paper Organisation
The remainder of this paper is structured as follows: 


\section{Related Work}

\subsection{Artificial Intelligence in High-Risk Domains}

Corrections, Judiciary, Healthcare

\subsection{Architectural Frameworks for Trustworthy Artificial Intelligence}

\subsection{FAIR Principles and Ethical data governance}

\subsection{Data Lakehouse Architectures (Medallion Model)}

\subsection{Fairness, Bias, and Causality in Machine Learning}

\subsection{Anonymization Techniques}
k-Anonymity, l-Diversity, t-Closeness, Differential Privacy

\subsection{Gaps in Current Literature}

No architectural integration

No continuous fairness+privacy evaluation

No causal HITL governance
\end{comment}

\section{Introduction}
The growing integration of AI-driven decision-making into public-sector and safety-critical environments has introduced new pressures on software architecture. Traditional architectural concerns, performance, scalability, interoperability, are no longer sufficient when system outputs directly influence individual rights, access to resources, or legal determinations. In domains such as corrections, public safety, and healthcare, the software architect is increasingly responsible for ensuring that data infrastructures uphold principles of fairness, privacy, causal validity, and regulatory compliance. These requirements redefine the architectural role from solely optimizing technical qualities to governing socio-technical risks embedded in the data lifecycle, motivating the need for structured, end-to-end data governance pipelines such as the one depicted in Figure~\ref{fig:arch}. As AI systems become embedded in these sensitive settings, mounting evidence shows that trustworthiness depends not only on accuracy but also on robustness, transparency, accountability, and the mitigation of systemic and historical biases \cite{li2021trustworthy, mitchell2019model}. Algorithmic tools such as COMPAS have demonstrated racial disparities that arise even when protected attributes are excluded from training \cite{angwin2016machine, dressel2018accuracy}. In healthcare, biased datasets can propagate inequities in diagnosis and treatment \cite{plos2022healthcare}.  For instance, 
Obermeyer et al. demonstrated that a widely-used commercial algorithm for managing patient care exhibited significant racial bias by using healthcare costs as a proxy for health needs, resulting in Black patients being systematically undertreated \cite{obermeyer2019dissecting}. Rajkomar et al. emphasize that ensuring fairness in healthcare ML requires attention throughout the entire development lifecycle \cite{rajkomar2018ensuring}. These cases illustrate that failures in data governance manifest as failures in system behavior, reinforcing the need for architectural frameworks that embed ethical safeguards \textit{by design} rather than as post-hoc patches.
\begin{figure*}[t]
    \centering
    \includegraphics[width=\textwidth]{arch - Copy.png}
    \caption{The FAIR-CARE Lakehouse pipeline architecture, illustrating the end-to-end governance flow across the Bronze, Silver, and Gold layers. Each stage embeds privacy, causal validity, and fairness mechanisms into the data lifecycle.}
    \label{fig:arch}
\end{figure*}

\subsection{Motivation: Architecting for High-Risk AI Systems}
High-risk AI systems are characterized by decisions with direct social, legal, or safety consequences. In such environments, data pipelines become critical architectural assets, as they mediate how information is collected, transformed, and exposed to downstream models. Ensuring the trustworthiness of these systems therefore requires moving beyond conventional performance-oriented ETL pipelines and explicitly managing non-functional requirements (NFRs) such as fairness, privacy preservation, lineage integrity, and auditability \cite{li2021trustworthy, chouldechova2017fair}.

Correctional datasets exemplify the challenges of high-risk governance. Recidivism forecasting models are susceptible to ``data bias,'' where patterns reflect historical policing practices rather than behavioral risk \cite{nij_context}. Privacy risks are equally substantial: combinations of quasi-identifiers such as release dates, geographic zones, and offense categories can re-identify individuals even after direct identifiers are removed \cite{sweeney2002k}. These issues highlight that ethical and regulatory requirements must be operationalized at the architectural level, not delegated to model developers or compliance teams in isolation.

\subsection{Problem Statement}
Conventional Extract--Transform--Load (ETL) and lakehouse architectures are optimized for throughput, schema evolution, and analytical readiness, but they are not designed to enforce ethical constraints throughout the data lifecycle. As a result, they exhibit several limitations:

\begin{itemize}
    \item \textbf{Fragmented Responsibility:} Ethical constraints such as bias mitigation, anonymization, and compliance are distributed across disconnected teams, with no architectural component providing end-to-end governance \cite{veulemans2024detecting}.
    
    \item \textbf{Causal Opacity:} Traditional pipelines treat correlations as sufficient signals for downstream learning, offering no mechanisms to identify or suppress spurious causal paths (e.g., Zip Code~$\rightarrow$ Race~$\rightarrow$ Risk) \cite{smuc2022causal}.
    
    \item \textbf{Regulatory Misalignment:} Standards such as GDPR anonymization, HIPAA Expert Determination, and CCPA functional separation are rarely embedded into pipeline logic; instead, compliance is validated only after processing \cite{gdpr2016}.
\end{itemize}

These limitations point to the need for a unified architectural framework that integrates fairness, privacy, lineage governance, and causal reasoning as first-class architectural citizens. Addressing these gaps requires reconceptualizing the role of the software architect as a steward of data ethics and accountable system behavior.

\subsection{Research Questions}
This work investigates how ethical governance can be embedded structurally into data pipelines for high-risk AI systems. We formulate the following research questions:

\begin{itemize}
    \item \textbf{RQ1:} How can we architect an end-to-end data pipeline that enforces ethical governance, fairness, privacy, and compliance at each stage of the Medallion lakehouse model?
    
    \item \textbf{RQ2:} Which technical components (e.g., causal analysis, differential privacy, synthetic data generation, bias mitigation) are required within each layer to satisfy CARE requirements without causing unacceptable utility loss?
    
    \item \textbf{RQ3:} How can we quantitatively assess the ethical readiness of datasets in high-risk domains by combining FAIR data principles with explicit ethical criteria into a composite, layer-aware metric?
\end{itemize}

\subsection{Contributions}
To address these challenges, this paper introduces the \textbf{FAIR-CARE Lakehouse}, a reference architecture that embeds FAIR data principles alongside the CARE ethical framework (Causality, Anonymity, Regulatory compliance, Ethics) across the Medallion lifecycle. Our contributions are as follows:

\begin{enumerate}
    \item \textbf{Reference Architecture:} We propose an extension of the Medallion Lakehouse (Bronze--Silver--Gold) that incorporates Privacy, Enhancing Technologies (PETs), causal inference mechanisms, and structured governance policies as architectural elements.
    
    \item \textbf{Algorithmic Framework:} We design a multi-stage pipeline that includes PII detection, Differential Privacy (DP), Synthetic Data Generation (SDG), causal structure learning, and human-in-the-loop validation, implemented as modular, reusable architectural components.
    
    \item \textbf{Evaluation Framework:} We introduce the \textbf{FAIR-CARE Score}, a composite, layer-specific metric that evaluates privacy leakage, fairness disparities, lineage completeness, and causal soundness to quantify ethical readiness.
    
    \item \textbf{Regulatory Compliance Mapping:} We demonstrate how architectural parameters (e.g., DP privacy budget $\epsilon$, k-anonymity thresholds, lineage granularity) can be configured to align with GDPR, HIPAA, and CCPA requirements.
    
    \item \textbf{Empirical Validation:} Using benchmark datasets from the NIJ Recidivism Forecasting Challenge \cite{nij_challenge} and widely studied fairness datasets, we illustrate how legacy correctional data can be transformed into a compliant, fair, and analytically valuable asset.
\end{enumerate}



\section{Related Work}

Research on trustworthy AI, privacy-preserving data engineering, and data lakehouse architectures has expanded significantly in the past decade, yet existing approaches remain fragmented and insufficient for high-risk AI systems. Prior work in AI ethics has extensively documented the harms arising from biased datasets and opaque model pipelines \cite{li2021trustworthy, mitchell2019model}. Studies on recidivism risk assessment, such as those analyzing COMPAS, show that models continue to reproduce racial disparities even when explicit sensitive attributes are removed \cite{angwin2016machine, dressel2018accuracy}. However, these analyses typically focus on model-level behavior rather than on the architectural structures that propagate bias. As a result, the literature does not offer a principled architectural mechanism for ensuring that fairness and causal integrity are preserved across the data lifecycle.

Similarly, research on regulatory compliance provides detailed interpretations of GDPR, HIPAA, and CCPA requirements, especially regarding anonymization and re-identification risk \cite{edpb2014anonymization, hipaa_method, ccpa2018}. Yet, existing engineering practices primarily apply these regulations through ad-hoc masking rules, access policies, or manual review processes, approaches that are brittle and difficult to scale. The lack of architectural enforcement means that compliance is evaluated only after data has already propagated through multiple systems, creating opportunities for leakage or misuse. None of these works provide architectural abstractions that embed compliance guarantees into the pipeline itself.

\begin{table*}[ht]
\centering
\caption{Comparison of Existing Approaches and the FAIR-CARE Lakehouse}
\begin{tabular}{p{3.6cm} p{3.8cm} p{4.0cm} p{4.2cm}}
\toprule
\textbf{Approach} & \textbf{Strengths in Literature} & \textbf{Limitations for High-Risk AI Systems} & \textbf{How FAIR-CARE Lakehouse Addresses the Gap} \\
\midrule

AI Ethics and Bias Studies &
Identifies bias propagation and fairness failures; provides evaluation metrics \cite{hardt2016equality}. &
Focuses on model-level fixes; no architectural prevention of bias in pipelines. &
Causal validation and fairness filtering embedded at Silver/Gold layers prevent bias before model training. \\

Regulatory Compliance (GDPR, HIPAA, CCPA) &
Clear definitions of anonymization, re-identification, and functional separation \cite{edpb2014anonymization}. &
Compliance enforced via policies or manual audits; not architecturally guaranteed. &
PETs, privacy budgets, and functional separation implemented as structural pipeline constraints. \\

Privacy-Preserving Data Engineering &
Provides DP, SDG, and anonymization techniques \cite{dwork2006calibrating}. &
Techniques applied ad-hoc; no lineage tracking or integration with fairness or causal checks. &
DP-SDG integration with lineage metadata and causal DAG validation within pipeline layers. \\

Causal Inference and Fairness &
Defines causal fairness frameworks and spurious-path detection \cite{kusner2017counterfactual}. &
Implemented at model stage; does not govern upstream transformations. &
Causal DAGs govern feature selection, confounder suppression, and transformation logic. \\

Data Lakehouse Architectures &
Robust schema management and scalable ETL/ELT \cite{databricks_medallion}. &
No ethical governance, privacy enforcement, or fairness auditing across layers. &
Extends Medallion architecture with CARE governance mechanisms and FAIR-CARE scoring. \\

\bottomrule
\end{tabular}
\end{table*}

Beyond corrections, healthcare has similarly demonstrated how algorithmic bias can systematically disadvantage vulnerable populations. Obermeyer et al. \cite{obermeyer2019dissecting} revealed that a commercial risk prediction algorithm used on over 200 million patients exhibited substantial racial bias, with Black patients requiring significantly more chronic conditions than White patients to receive the same risk score. Rajkomar et al. \cite{rajkomar2018ensuring} provide a comprehensive framework for 
ensuring fairness throughout the ML lifecycle in healthcare, emphasizing the need for diverse training data and continuous monitoring—principles directly applicable to correctional systems.

Work on fairness and causal inference has introduced metrics such as Demographic Parity and Equalized Odds \cite{hardt2016equality}, alongside causal frameworks such as Counterfactual Fairness \cite{kusner2017counterfactual}. While these contributions are theoretically powerful, they are typically implemented at the modeling stage, after data transformation has already occurred. Existing ML pipelines do not ensure that upstream transformations preserve causal validity or prevent the introduction of spurious relationships. Consequently, fairness interventions are reactive rather than structural, and pipeline-level sources of bias remain unaddressed. Privacy-preserving data engineering research provides foundational techniques including $k$-anonymity, $l$-diversity, and Differential Privacy (DP) \cite{sweeney2002k, machanavajjhala2007l, dwork2006calibrating}. More recent work demonstrates that combining DP with Synthetic Data Generation (SDG) can preserve utility while reducing re-identification risk \cite{schneider2022investigating}. However, these techniques are rarely operationalized as architectural layers with lineage guarantees, policy enforcement, and measurable governance outcomes. They are typically one-off preprocessing steps rather than continuous, auditable transformations within a governed pipeline.

Data lakehouse architectures, particularly the Medallion pattern (Bronze--Silver--Gold), offer strong support for schema evolution, data cleaning, and analytical readiness \cite{databricks_medallion}. Yet, their existing implementations do not natively address fairness, privacy, causal soundness, or regulatory compliance. Governance features such as audit logs and access control exist, but ethical and regulatory safeguards are not woven into the transformation logic of each layer. Thus, while the lakehouse paradigm is architecturally promising, it lacks the mechanisms required for legally compliant, ethically governed, high-risk AI systems.

Taken together, the existing body of work provides valuable conceptual tools but lacks a unified architecture that:  
(1) embeds privacy and fairness as first-class architectural concerns;  
(2) integrates causal validation and PETs directly into Medallion-layer transformations;  
(3) maps technical settings to regulatory frameworks; and  
(4) offers a measurable governance metric for dataset readiness.  

The FAIR-CARE Lakehouse addresses these gaps by providing an architecture that operationalizes ethical governance as an intrinsic property of the data pipeline rather than an external process.

\begin{comment}
\section{Methodology}

\subsection{Architectural Overview}
FAIR-CARE Reference Architecture - High-level diagram; architectural style; components.

Layer 1: Bronze (Raw Ingestion with FAIR-CARE Enhancements)
Provenance tracking

PII/SPI auto-detection

Baseline bias estimation
Layer 2: Silver (Cleansing, Anonymization, and Causal Governance)

Multi-method anonymization engine

Construction of causal DAGs

HITL causal validation interface

Layer 3: Gold (Fair & Feature-Ready Outputs)

Bias mitigation techniques

Fairness metrics computation

Embedding + RAG/CAG preparation
\end{comment}

\section{Methodology}

\subsection{Architectural Overview}
We propose the \textbf{FAIR-CARE Lakehouse}, a reference architecture that integrates FAIR data principles with the CARE ethical framework (Causality, Anonymity, Regulatory compliance, Ethics). As illustrated in Figure \ref{fig:arch}, the system extends the standard Medallion pattern (Bronze--Silver--Gold) \cite{databricks_medallion} by embedding privacy-enhancing technologies (PETs) and causal inference engines directly into the transformation logic.

The architecture is implemented using a hybrid distributed computing engine. While Apache Spark is utilized for the ``heavy lifting'' of ETL and schema enforcement in the Bronze layer, we utilize modular Python components in the Silver and Gold layers. This division allows for the integration of specialized libraries required for causal discovery algorithms and synthetic data generation.


\subsection{Layered Design and Transformations}

\subsubsection{Bronze Layer: The Quarantine Zone}
The Bronze layer serves as the immutable system of record. Data enters via Spark Structured Streaming, ensuring ACID compliance through Delta Lake.
\begin{itemize}
    \item \textbf{PII Detection:} Upon ingestion, an automated scanner (utilizing Microsoft Presidio and Regex patterns) scans columns for Personally Identifiable Information (PII) and quasi-identifiers (e.g., Zip Code, Release Date).
    \item \textbf{Pseudonymization:} Direct identifiers are hashed using a salted key stored in a hardware security module (HSM), implementing the ``Functional Separation'' required by CCPA \cite{ccpa2018}.
    \item \textbf{Provenance:} Metadata regarding source, ingestion timestamp, and schema hash are logged to the Unity Catalog.
\end{itemize}

\subsubsection{Silver Layer: The Privacy and Causal Engine}
The Silver layer transforms risky, pseudonymized data into research-ready assets using isolated Python tasks.
\begin{itemize}
    \item \textbf{Differential Privacy (DP):} We implement $\epsilon$-Differential Privacy using the Laplace mechanism to perturb numeric columns. A privacy budget ($\epsilon$) is tracked by the Control Plane. Should the budget be exhausted, the dataset is locked to prevent reconstruction attacks \cite{dwork2006calibrating}.
    \item \textbf{Adaptive Anonymization:} For non-numeric attributes, the system dynamically selects between $k$-anonymity, $l$-diversity, and $t$-closeness based on the sensitivity of the data and the distribution of quasi-identifiers, ensuring granular protection against re-identification.
    \item \textbf{Causal Discovery:} Unlike standard ETL, this layer constructs a Causal Directed Acyclic Graph (DAG) using algorithms such as PC or NOTEARS via the \textit{CausalNex} library \cite{causalnex}. This graph identifies confounders (e.g., Race $\rightarrow$ SES $\rightarrow$ Recidivism) to prevent the modeling of spurious correlations.
\end{itemize}

\subsubsection{Gold Layer: Fairness and Feature Store}
The Gold layer produces the final feature store.
\begin{itemize}
    \item \textbf{Markov Blanket Selection:} Features are filtered based on the causal DAG; only features in the Markov Blanket of the target variable are retained, ensuring causal sufficiency \cite{dowhy_intro}.
    \item \textbf{Bias Mitigation:} We apply pre-processing algorithms from AIF360, such as \textit{Reweighing} or \textit{Disparate Impact Remover}, to correct historical imbalances \cite{aif360_github}.
    \item \textbf{Vector Embedding:} For Retrieval-Augmented Generation (RAG) applications, text fields (e.g., case narratives) are vectorized and stored in Qdrant, tagged with their fairness certification.
\end{itemize}

\subsection{The FAIR-CARE Score}
To quantify the ethical readiness of the data, we define a composite score calculated at runtime. The score is a weighted sum of layer-specific metrics:

\begin{equation}
    \text{Score}_{FC} = w_B S_B + w_S S_S + w_G S_G
\end{equation}

Where the sub-scores are defined as:

\begin{enumerate}
    \item \textbf{Bronze Score ($S_B$):} Measures ingestion quality.
    $$S_B = \frac{1}{3}(\text{Provenance}_{\%} + \text{PII}_{\text{Recall}} + \text{Quality}_{\text{Base}})$$
    
    \item \textbf{Silver Score ($S_S$):} Measures privacy-utility balance.
    $$S_S = \frac{1}{4}(\text{Anon}_{\text{Str}} + \text{Util}_{\text{Ret}} + \text{Causal}_{\text{Valid}} + \text{HITL}_{\text{Appr}})$$
    Here, $\text{Anon}_{\text{Str}}$ is derived from the achieved privacy loss ($1 - \frac{\epsilon}{\epsilon_{max}}$) and $k$-anonymity thresholds.
    
    \item \textbf{Gold Score ($S_G$):} Measures downstream fairness.
    $$S_G = \frac{1}{3}(\text{Fairness}_{\text{Met}} + \text{Feat}_{\text{Qual}} + \text{Util}_{\text{Pred}})$$
    $\text{Fairness}_{\text{Met}}$ aggregates pass/fail rates for Demographic Parity and Equalized Odds.
\end{enumerate}

Algorithm \ref{alg:privacy_transformer} demonstrates the logic for the ``Privacy Transformer,'' a key component in the Silver layer that enforces these constraints.

\begin{algorithm}[h]
\caption{Privacy Preserving Transformation (Silver Layer)}\label{alg:privacy_transformer}
\begin{algorithmic}[1]
\REQUIRE Dataset $D$, Config $\{k, l, t, \epsilon, \text{technique}\}$
\ENSURE Anonymized Dataset $D_{safe}$ satisfying Privacy Constraints
\STATE \textbf{Initialize} AnonymizationEngine with config
\IF{$\text{technique} == \text{differential\_privacy}$}
    \STATE $D_{safe} \leftarrow$ \textbf{ApplyLaplaceMechanism}($D$, $\epsilon$)
\ELSIF{$\text{technique} == \text{k\_anonymity}$}
    \STATE $D_{safe} \leftarrow$ \textbf{GeneralizeAndSuppress}($D$, $k$)
\ELSIF{$\text{technique} == \text{l\_diversity}$}
    \STATE $D_{safe} \leftarrow$ \textbf{EnsureDiverseSensitive}($D$, $k$, $l$)
\ELSIF{$\text{technique} == \text{t\_closeness}$}
    \STATE $D_{safe} \leftarrow$ \textbf{EnsureDistributionCloseness}($D$, $k$, $t$)
\ENDIF
\STATE \textbf{Compute} $S_S$ (Silver Score)
\STATE \textbf{Return} $D_{safe}$
\end{algorithmic}
\end{algorithm}


% \section{Experimental Evaluation}

% \subsection{Datasets}
% We validate the architecture using four high-dimensional benchmark datasets widely used in fairness research \cite{li2021trustworthy, nij_challenge}:
% \begin{itemize}
%     \item \textbf{NIJ Recidivism Challenge:} A rich correctional dataset containing 25,000+ records with sensitive attributes (supervision level, drug tests) and recidivism outcomes \cite{nij_context}.
%     \item \textbf{COMPAS:} The canonical dataset for analyzing racial bias in risk assessment \cite{angwin2016machine}.
%     \item \textbf{UCI Adult \& German Credit:} Standard benchmarks for income and credit scoring disparity.
% \end{itemize}

% \subsection{Experimental Setup}
% Experiments were conducted on a cluster running \textbf{Ray on Spark} \cite{ray_on_spark} with 4 worker nodes (16 vCPUs each). The pipeline was configured with strict GDPR-aligned parameters: Privacy Budget $\epsilon < 1.0$ and $k$-anonymity threshold $k=5$. We utilized \textit{ARX} for privacy metrics, \textit{DoWhy} for causal refutation, and \textit{AIF360} for bias mitigation.

% We compare the \textbf{FAIR-CARE Pipeline} against two baselines:
% \begin{enumerate}
%     \item \textbf{Baseline A (Naive ETL):} Standard pseudonymization (hashing IDs) with no statistical anonymization or causal filtering.
%     \item \textbf{Baseline B (Masking):} A pipeline applying static data masking (suppression) without synthetic generation.
% \end{enumerate}

% \subsection{Results}

% \subsubsection{Fairness vs. Utility Trade-off}
% Table \ref{tab:results} summarizes the performance on the NIJ dataset. The FAIR-CARE pipeline, utilizing Causal Discovery to remove spurious confounders (e.g., zip code proxies), reduced the \textit{Equalized Odds Difference} (EOD) by 60\% compared to Baseline A. While synthetic generation introduced a marginal drop in model accuracy (AUC decreased by 0.03), the privacy guarantee ($\epsilon=0.8$) prevents re-identification, satisfying the ``Expert Determination'' standard of HIPAA.

% \begin{table}[htbp]
% \caption{Comparison of Fairness and Utility on NIJ Dataset}
% \begin{center}
% \begin{tabular}{|l|c|c|c|c|}
% \hline
% \textbf{Pipeline} & \textbf{AUC} $\uparrow$ & \textbf{EOD} $\downarrow$ & \textbf{Priv. Risk} $\downarrow$ & \textbf{Score}_{FC} \\
% \hline
% Baseline A (Naive) & \textbf{0.74} & 0.18 & High (Linkable) & 0.45 \\
% Baseline B (Mask) & 0.61 & 0.12 & Low & 0.62 \\
% \textbf{FAIR-CARE} & 0.71 & \textbf{0.07} & \textbf{Provable ($\epsilon$)} & \textbf{0.88} \\
% \hline
% \end{tabular}
% \label{tab:results}
% \end{center}
% \end{table}

% \subsubsection{Causal Validity}
% Using the \textit{DoWhy} refutation test, we validated the causal graphs generated in the Silver Layer. The inclusion of a causal filter rejected 14\% of features that were highly correlated with the target but statistically determined to be spurious (non-causal) proxies for race. This structurally prevents the model from learning bias, a capability absent in standard feature selection methods.
\subsection{Experimental Evaluation}

\subsubsection{Datasets}
We validate the FAIR-CARE Lakehouse architecture using four widely studied benchmark datasets in fairness and high-risk decision-making research \cite{li2021trustworthy, nij_challenge}. These datasets span correctional forecasting, credit scoring, and income prediction, domains where fairness, privacy, and causal integrity are critical:
\begin{itemize}
    \item \textbf{NIJ Recidivism Challenge:} A correctional dataset containing over 25{,}000 records with sensitive behavioral and supervision attributes, including ground-truth recidivism outcomes \cite{nij_context}.
    \item \textbf{COMPAS:} A canonical benchmark for assessing racial disparities in algorithmic risk assessment \cite{angwin2016machine}.
    \item \textbf{UCI Adult \& German Credit:} Standard fairness benchmarks frequently used to study socioeconomic bias in income and credit scoring.
\end{itemize}

These datasets provide a representative evaluation across multiple high-risk application contexts. Their characteristics are summarized in Table \ref{tab:dataset_characteristics}.

Table~\ref{tab:dataset_characteristics} summarizes their key properties, 
highlighting the demographic imbalances and class skew that motivate our 
fairness-preserving architecture.

\begin{table}[htbp]
\centering
\caption{Benchmark Dataset Characteristics and Bias Indicators}
\label{tab:dataset_characteristics}
\small
\begin{tabular}{lrccp{2.5cm}}
\hline
\textbf{Dataset} & \textbf{Size} & \textbf{Pos.} & \textbf{Sens. Attr.} & \textbf{Primary Bias} \\
\hline
NIJ Recidivism & 25,690 & 30\% & Race, Gender & Gender skew (88\% Male), Racial imbalance (57\% Black) \\
\hline
COMPAS & 7,214 & 45\% & Race, Sex & Racial disparity (52\% African-American, 34\% Caucasian) \\
\hline
Adult Census & 32,561 & 24\% & Race, Sex & Race-income correlation (85\% White, skewed toward <=50K) \\
\hline
German Credit & 1,000 & 30\% & Age, Sex & Small sample, age skew (mean=35, right-tailed), 69\% Male \\
\hline
\end{tabular}
\end{table}

These datasets exhibit varying degrees of class imbalance (24-45\% positive class) and demographic skew. The NIJ and COMPAS datasets originate from actual correctional risk assessment systems, making them particularly relevant for validating our architecture's real-world applicability. Adult Census and German Credit provide cross-domain validation in financial decision-making contexts, where similar fairness concerns arise. Figure \ref{fig:eda} illustrates the demographic distributions and imbalances across the four benchmark datasets.

\begin{figure}[t]
    \centering
    \includegraphics[width=\columnwidth]{images/fig_demographic.png}
    \caption{Demographic characteristics and Class Imbalance across benchmark datasets. (a) NIJ Recidivism shows extreme gender imbalance. (b) COMPAS exhibits racial disparity. (c) Adult Census shows correlation between race and income. (d) Class imbalance varies significantly across datasets.}
    \label{fig:eda}
\end{figure}

\subsubsection{Experimental Setup}

Experiments were executed on a Spark-based cluster configured with four worker nodes (16 vCPUs each). Spark handles the ingestion and Bronze-layer transformations, while the Silver and Gold layer components for causal discovery, DP-SGD synthetic data generation, and privacy verification are implemented as modular Python workflows orchestrated within the Spark-driven pipeline, without deploying the Ray-based execution model that is proposed in the reference architecture for future large-scale deployments.

To align with regulatory standards, the pipeline was configured with strict parameters:
\begin{itemize}
    \item Privacy Budget: $\epsilon < 1.0$ (GDPR-aligned threshold),
    \item Minimum anonymity guarantee: $k=5$,
    \item Anonymization: Adaptive application of $k$-anonymity, $l$-diversity, and $t$-closeness,
    \item Causal refutation: Performed using \textit{DoWhy},
    \item Bias analysis: Conducted using \textit{AIF360},
    \item Privacy metrics: Computed with \textit{ARX}.
\end{itemize}

We evaluate the FAIR-CARE pipeline relative to two baselines representing common industry practices:
\begin{enumerate}
    \item \textbf{Baseline A (Naive ETL):} Standard pseudonymization using hashed identifiers, without statistical anonymization, causal filtering, or fairness enforcement.
    \item \textbf{Baseline B (Masking):} Static suppression-based masking applied prior to modeling, without synthetic generation or causal validation.
\end{enumerate}

These baselines reflect the typical configurations found in production ETL pipelines and allow us to assess how architectural governance mechanisms influence privacy, fairness, and utility.


\begin{comment}
    
\subsection{Benchmark Comparisons}




\section{Results and Discussion}

Evaluation Framework

Layer-Specific Metrics

Weighted Score Function

Visualizations

Legal and Ethical Analysis

Discussion



\section{Limitations}




\section{Conclusion and Future Work}
\label{sec:conclusion-future-work}
Summary of Contributions

Future works
\cite{turing1950computing}
\end{comment}


\section{Results and Discussion}

\subsection{Experimental Outcomes}
Our evaluation examines how the FAIR-CARE Lakehouse balances the three pillars of the CARE framework—\textit{Privacy}, \textit{Fairness}, and \textit{Utility}. Privacy is quantified through the achieved Differential Privacy budget ($\epsilon$) and $k$-anonymity thresholds; fairness is assessed using Equalized Odds Difference (EOD) and Demographic Parity Difference (DPD); and utility is measured through AUC. Together, these metrics enable holistic assessment of whether the architecture supports ethically governed AI in high-risk settings.


\subsubsection{Benchmark Comparisons Across Fairness Datasets}
We applied the FAIR-CARE pipeline to three widely used fairness datasets: COMPAS, Adult Census, and German Credit. Across all benchmarks, the architecture substantially reduced fairness disparities relative to a naive ETL baseline.

Table~\ref{tab:ablation} presents an ablation study on the COMPAS dataset. The best FAIR‑CARE configurations (Configs A and B) reduce EOD from 0.35 to 0.25 and DPD from 0.28 to 0.20 compared to the baseline, while maintaining utility above 0.85. This pattern shows that introducing $k$-anonymity and differentially private synthetic data generation mitigates disparities at a modest accuracy cost (from 1.00 down to 0.87–0.94), a trade-off aligned with the safeguards expected in high-risk deployments. This aligns with prior findings that Differential Privacy and Synthetic Data Generation can mitigate historical biases by smoothing extreme distributions \cite{veulemans2024detecting}. The utility remains high (0.87–0.94), representing a modest accuracy trade-off consistent with ethical safeguards required in high-risk deployments.

\begin{figure}[t]
    \centering
    \includegraphics[width=\columnwidth]{images/fig3_benchmark_datasets.png}
    \caption{Multi-dataset benchmarking of Bronze (SB), Silver (SS), Gold (SG), and overall FAIR-CARE Scores across Adult, COMPAS, German Credit, and NIJ datasets. Higher bars indicate greater ethical readiness at each layer.}
    \label{fig:benchmark-layer-scores}
\end{figure}

Figure~\ref{fig:benchmark-layer-scores} summarizes layer-wise and composite FAIR-CARE Scores across all four datasets, showing that the architecture consistently improves Bronze, Silver, and Gold governance while keeping the overall FAIR-CARE Score in a high-readiness regime.


\begin{table}[htbp]
\caption{Ablation Study: Impact of FAIR-CARE Layers on Adult Dataset}
\begin{center}
\begin{tabular}{|l|c|c|c|c|}
\hline
\textbf{Configuration} & \textbf{Score}$_{FC}$ & \textbf{SS} (Silver) & \textbf{Privacy} & \textbf{Utility} \\
\hline
Baseline (No CARE) & 0.72 & 1.00 & Low & 1.00 \\
Config A ($k$-anon) & 0.74 & 0.94 & Med & 0.82 \\
Config B (Diff. Priv) & 0.78 & 1.00 & High & 1.00 \\
Config C (Causal) & 0.74 & 1.00 & Med & 1.00 \\
\hline
\multicolumn{5}{l}{\scriptsize scores from exp1.csv evaluation.}\\
\end{tabular}
\label{tab:ablation}
\end{center}
\end{table}

The impact of these configurations on the composite FAIR-CARE Score is visualized in Figure \ref{fig:ablation}. While individual fairness metrics improve, the overall score balances this against utility loss.

\begin{figure}[t]
    \centering
    \includegraphics[width=\columnwidth]{images/fig1_ablation_faircare.png}
    \caption{Ablation Study: FAIR-CARE Score by Configuration (Adult Dataset). The architecture demonstrates a progression towards robust governance (approaching 0.80) as privacy and fairness modules are activated.}
    \label{fig:ablation}
\end{figure}

Figure~\ref{fig:eda} illustrates the demographic imbalances present in the benchmark datasets. The gender skew (67-88\% male across datasets) and racial composition variations (from 52\% African-American in COMPAS to 85\% White in Adult) demonstrate why naive ETL pipelines fail: models trained on these distributions will inherently encode historical biases~\cite{obermeyer2019dissecting, dressel2018accuracy}. 
The FAIR-CARE architecture addresses this through three mechanisms: (1) PII detection and pseudonymization in the Bronze Layer, (2) causal filtering to remove demographic proxies in the Silver Layer, and (3) reweighing and bias mitigation in the Gold Layer.

\subsubsection{Causal Validity Across Pipelines}
As part of the Silver Layer processing, we examined the impact of causal filtering using DoWhy refutation tests. The causal discovery stage explicitly validated the directionality of relationships between sensitive attributes and outcomes. This capability allows the pipeline to flag potential spurious correlations (e.g., ZIP codes acting as proxies for race) that standard ETL processes might blindly ingest. The inclusion of causal validation contributes meaningfully to the composite FAIR-CARE score by penalizing models that rely on unverified assumptions.



\subsection{Case Study: The NIJ Recidivism Challenge}
To assess the architecture in a high-risk, real-world context, we evaluated the NIJ Recidivism Challenge dataset \cite{nij_challenge}, utilizing Logistic Regression and ROC AUC as the primary utility metric:

\begin{itemize}
    \item \textbf{Baseline (Legacy):} Direct modeling on raw data yielded a high AUC of 0.92, indicating strong predictive signal, but presented substantial privacy leakage and high fairness disparity.
    \item \textbf{Silver Layer (Privacy):} Applying the Privacy Preservation module with $\epsilon = 1.0$ maintained a robust AUC of 0.91, showing that utility can be almost fully preserved even under strict privacy constraints.
    \item \textbf{Gold Layer (Fairness):} After bias mitigation via AIF360 Reweighing, we achieved a significant reduction in Statistical Parity Difference, demonstrating that the architecture can actively correct historical biases with minimal loss in predictive utility (Final AUC $\approx$ 0.91).
\end{itemize}

Although the FAIR-CARE pipeline does not maximize predictive accuracy, it produces an \textit{ethically robust} model with demonstrable protections against discriminatory or privacy-violating behavior, as shown in Figure~\ref{fig:nij_case_study}, an essential property for correctional applications.

\begin{figure}[t]
    \centering
    \includegraphics[width=\columnwidth]{images/fig_nij_case_study.png}
    \caption{Case Study Results on NIJ Recidivism Dataset. The chart illustrates the trade-off between predictive utility (AUC) and fairness disparity. While the Baseline model achieves high AUC (0.92), it suffers from high disparity (EOD $\approx$ 0.18). The Gold Layer sacrifices some predictive power (AUC $\approx$ 0.91) to achieve a fairer model (EOD $\approx$ 0.07).}
    \label{fig:nij_case_study}
\end{figure}

\subsection{FAIR-CARE Score Analysis}
The FAIR-CARE Score consolidates ethical and technical metrics across the Medallion layers. As shown in Table~\ref{tab:ablation}, the naive baseline scores lowest (0.72) due to the absence of anonymization and causal safeguards. The full FAIR-CARE configuration (Config B) achieves the highest score of 0.78, approaching the 0.80 threshold indicative of ``Robust Governance.'' This demonstrates that ethical readiness is not a byproduct of isolated privacy or fairness techniques, but emerges from coordinated architectural constraints across Bronze, Silver, and Gold layers.

\begin{figure}[t]
    \centering
    \includegraphics[width=\columnwidth]{images/fig4_benchmark_techniques.png}
    \caption{FAIR-CARE Score by privacy technique (left) and corresponding utility--privacy trade-off (right). Differential Privacy (dp) attains the highest FAIR-CARE Score (0.685) with the best utility retention, but also exhibits the highest residual privacy risk, whereas $k$-anonymity, $l$-diversity, and $t$-closeness achieve slightly lower scores (0.663--0.674) with comparable utility and reduced estimated privacy risk.}
    \label{fig:benchmark-techniques}
\end{figure}


Figure~\ref{fig:benchmark-techniques} compares FAIR-CARE Scores and utility--privacy trade-offs across four privacy techniques, showing that Differential Privacy achieves the highest composite score and utility retention but at the cost of higher residual privacy risk, while classical anonymization methods offer marginally lower FAIR-CARE Scores with similar utility and lower estimated privacy exposure.



\subsection{Legal and Ethical Compliance}
We now evaluate the architectural implications in the context of regulatory frameworks, addressing \textbf{RQ3}:

\begin{itemize}
    \item \textbf{GDPR:} Differential Privacy ($\epsilon \le 1.0$) directly supports the GDPR definition of ``anonymization'' by preventing singling out, linkability, and inference \cite{edpb2014anonymization}.
    \item \textbf{HIPAA:} The Silver Layer implements the ``Expert Determination'' pathway through DP mechanisms and statistical disclosure control, ensuring that residual re-identification risk is ``very small'' \cite{hipaa_method}.
    \item \textbf{CCPA/CPRA:} Functional separation between the Bronze (identifier keys) and Gold (analytic features) layers ensures that downstream consumers cannot reconstruct identity, satisfying statutory separation requirements.
\end{itemize}

Collectively, these results demonstrate that the FAIR-CARE Lakehouse not only improves fairness and privacy performance but also provides a principled architectural mechanism for satisfying regulatory mandates in high-risk AI systems.


\section{Threats to Validity}
Despite the structured design of the FAIR-CARE Lakehouse and its alignment with regulatory and ethical principles, several threats to validity must be acknowledged.

\subsection{Internal Validity}
Internal validity concerns whether the observed improvements can be reliably attributed to the architecture.
\begin{itemize}
    \item \textbf{Causal Graph Correctness:} The causal graph construction in the Silver Layer depends on domain expert elicitation and algorithmic discovery (e.g., PC, NOTEARS). These algorithms are sensitive to hyperparameter choices and may be influenced by unobserved confounders. If the initial structure is flawed, subsequent filtering may be ineffective. We mitigated this via refutation tests (\textit{DoWhy}), but automated causal discovery remains an open challenge.
    \item \textbf{Sensitivity to Privacy Parameters:} The Differential Privacy mechanisms (e.g., Laplace noise) introduce stochasticity. Small deviations in the privacy budget ($\epsilon$) or clipping thresholds can yield different utility-fairness trade-offs. While we fixed seeds for reproducibility, the inherent randomness of DP means that single-run results may vary slightly.
\end{itemize}

\subsection{External Validity}
External validity pertains to the generalizability of our findings.
\begin{itemize}
    \item \textbf{Data Modality:} Our evaluation focused exclusively on structured tabular datasets. Unstructured modalities (video, text narratives) require distinct privacy mechanisms (e.g., redaction, blurring) not currently implemented in our Silver Layer.
    \item \textbf{Socio-Legal Context:} The datasets used (COMPAS, NIJ, US Census) originate from the U.S. context. Bias patterns and regulatory requirements may differ significantly in international domains (e.g., EU healthcare or social services), meaning the specific causal structures observed here may not transfer directly.
\end{itemize}

\subsection{Construct Validity}
Construct validity addresses whether our metrics accurate reflect ethical constructs.
\begin{itemize}
    \item \textbf{Metric Limitations:} We assessed fairness using group-based metrics (DPD, EOD). While standard, these do not capture individual fairness or long-term societal impact. Similarly, \textit{utility} was operationalized as AUC, which does not capture calibration or interpretability requirements.
    \item \textbf{FAIR-CARE Score Subjectivity:} The composite score's weighting scheme ($w_S, w_G$) is architecturally defined. While meaningful for comparison, different stakeholder priorities might require alternative weightings (e.g., prioritizing privacy over fairness).
\end{itemize}

\subsection{Conclusion Validity}
Conclusion validity relates to the statistical strength of our claims.
\begin{itemize}
    \item \textbf{Statistical Power:} Our quantitative evaluation is based on a limited number of benchmark datasets ($N=4$). While we observed consistent trends, some paired comparisons lacked statistical significance at $p < 0.05$.
    \item \textbf{Real-World Robustness:} The fairness gains observed in static benchmarks may differ in production environments subject to distribution shift (data drift) or adversarial behavior. Future work must validate the architecture's resilience to dynamic data updates.
\end{itemize}

\subsection*{Artifact Availability}
To support reproducibility, we provide an anonymized code repository with the full FAIR-CARE pipeline implementation and experiment scripts at the following URL (anonymized for review): \url{https://anonymous.4open.science/r/XXXX}. 



\section{Conclusion and Future Work}
We have presented the \textbf{FAIR-CARE Lakehouse}, a reference architecture that transforms the role of the software architect from a builder of systems to a guardian of ethics. By embedding Differential Privacy, Synthetic Data Generation, and Causal Inference into the Medallion data lifecycle, we demonstrate that compliance and fairness need not be post-hoc afterthoughts.

Our experiments on the NIJ and COMPAS datasets confirm that it is possible to build pipelines that satisfy strict regulatory requirements (GDPR/HIPAA) while maintaining sufficient utility for decision support. The introduction of the \textbf{FAIR-CARE Score} provides organizations with a quantifiable metric to benchmark their ethical data readiness.

\subsection{Future Work: Federated Learning}
A natural extension of this architecture is \textbf{Federated Learning (FL)}. In correctional contexts, data is often siloed across state departments. Future iterations will integrate frameworks like \textit{NVFlare} or \textit{Ray Fed} to allow model training across these silos without data centralization, offering a higher tier of privacy-by-design.


\begin{comment}
\section*{Acknowledgment}
This work is supported by UIDB/04516/2020 of NOVA Laboratory for Computer Science and Informatics (NOVA LINCS), funded by FCT.IP. It also receives funding from the COMPETE2030 program under the Horus 360 iOMS NextGen project (COMPETE2030-FEDER-01316200).
\end{comment}

\printbibliography

\end{document}
